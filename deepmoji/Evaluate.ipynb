{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://github.com/msang/hateval/blob/master/SemEval2019-Task5/evaluation/evaluation.py\n",
    "#author: msang\n",
    "\n",
    "#!/usr/bin/env python\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import io\n",
    "\n",
    "def isfloat(value):\n",
    "  try:\n",
    "    float(value)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_file(path, correct_number_of_columns):\n",
    "    f = open(path, 'r', encoding=\"utf8\")\n",
    "    first_line = f.readlines()[0].split(\"\\t\")\n",
    "    f.close()\n",
    "    if (len(first_line) != correct_number_of_columns):\n",
    "        sys.exit('Column format problem.')\n",
    "    if (isfloat(first_line[0])):\n",
    "        has_header = 0\n",
    "    else:\n",
    "        has_header = 1\n",
    "    return has_header\n",
    "\n",
    "\n",
    "def evaluate_a(pred,gold):\n",
    "    levels = [\"HS\"]\n",
    "\n",
    "    ground_truth = pd.read_csv(gold, sep=\"\\t\", names=[\"ID\", \"Tweet-text\", \"HS\", \"TargetRange\", \"Aggressiveness\"],\n",
    "                               skiprows=check_file(gold, 5),\n",
    "                               converters={0: str, 1: str, 2: int, 3: int, 4: int}, header=None)\n",
    "\n",
    "    predicted = pd.read_csv(pred, sep=\"\\t\", names=[\"ID\"] + levels , skiprows=check_file(pred, 2),\n",
    "                            converters={0: str, 1: int}, header=None)\n",
    "\n",
    "    # Check length files\n",
    "    if (len(ground_truth) != len(predicted)):\n",
    "        sys.exit('Prediction and gold data have different number of lines.')\n",
    "\n",
    "    # Check predicted classes\n",
    "    for c in levels:\n",
    "        gt_class = list(ground_truth[c].value_counts().keys())\n",
    "        if not (predicted[c].isin(gt_class).all()):\n",
    "            sys.exit(\"Wrong value in \" + c + \" prediction column.\")\n",
    "\n",
    "    data = pd.merge(ground_truth, predicted, on=\"ID\")\n",
    "\n",
    "    if (len(ground_truth) != len(data)):\n",
    "        sys.exit('Invalid tweet IDs in prediction.')\n",
    "\n",
    "    # Compute Performance Measures HS\n",
    "    acc_hs = accuracy_score(data[\"HS_x\"], data[\"HS_y\"])\n",
    "    p_hs, r_hs, f1_hs, support = precision_recall_fscore_support(data[\"HS_x\"], data[\"HS_y\"], average = \"macro\")\n",
    "\n",
    "    return acc_hs, p_hs, r_hs, f1_hs\n",
    "\n",
    "def evaluate_b(pred,gold):\n",
    "    levels = [\"HS\", \"TargetRange\", \"Aggressiveness\"]\n",
    "\n",
    "    ground_truth = pd.read_csv(gold, sep=\"\\t\", names=[\"ID\", \"Tweet-text\", \"HS\", \"TargetRange\", \"Aggressiveness\"],\n",
    "                               skiprows=check_file(gold, 5),\n",
    "                               converters={0: str, 1: str, 2: int, 3: int, 4: int}, header=None)\n",
    "\n",
    "    predicted = pd.read_csv(pred, sep=\"\\t\", names=[\"ID\"] + levels , skiprows=check_file(pred, 4),\n",
    "                            converters={0: str, 1: int, 2: int, 3: int}, header=None)\n",
    "\n",
    "    # Check length files\n",
    "    if (len(ground_truth) != len(predicted)):\n",
    "        sys.exit('Prediction and gold data have different number of lines.')\n",
    "\n",
    "    # Check predicted classes\n",
    "    for c in levels:\n",
    "        gt_class = list(ground_truth[c].value_counts().keys())\n",
    "        if not (predicted[c].isin(gt_class).all()):\n",
    "            sys.exit(\"Wrong value in \" + c + \" prediction column.\")\n",
    "\n",
    "    data = pd.merge(ground_truth, predicted, on=\"ID\")\n",
    "\n",
    "    if (len(ground_truth) != len(data)):\n",
    "        sys.exit('Invalid tweet IDs in prediction.')\n",
    "\n",
    "    # Compute Performance Measures\n",
    "    acc_levels = dict.fromkeys(levels)\n",
    "    p_levels = dict.fromkeys(levels)\n",
    "    r_levels = dict.fromkeys(levels)\n",
    "    f1_levels = dict.fromkeys(levels)\n",
    "    for l in levels:\n",
    "        acc_levels[l] = accuracy_score(data[l + \"_x\"], data[l + \"_y\"])\n",
    "        p_levels[l], r_levels[l], f1_levels[l], _ = precision_recall_fscore_support(data[l + \"_x\"], data[l + \"_y\"], average=\"macro\")\n",
    "    macro_f1 = np.mean(list(f1_levels.values()))\n",
    "\n",
    "    # Compute Exact Match Ratio\n",
    "    check_emr = np.ones(len(data), dtype=bool)\n",
    "    for l in levels:\n",
    "        check_label = data[l + \"_x\"] == data[l + \"_y\"]\n",
    "        check_emr = check_emr & check_label\n",
    "    emr = sum(check_emr) / len(data)\n",
    "\n",
    "    return macro_f1, emr, acc_levels, p_levels, r_levels, f1_levels\n",
    "\n",
    "def write_eval(name):\n",
    "    # https://github.com/Tivix/competition-examples/blob/master/compute_pi/program/evaluate.py\n",
    "    # as per the metadata file, name of the output file is the argument\n",
    "\n",
    "    [input_dir, output_dir] = [\"input\", \"output\"]\n",
    "\n",
    "    # unzipped submission data is always in the 'res' subdirectory\n",
    "    # https://github.com/codalab/codalab-competitions/wiki/User_Building-a-Scoring-Program-for-a-Competition#directory-structure-for-submissions\n",
    "\n",
    "\n",
    "    ref_dir = os.path.join(input_dir, 'ref')\n",
    "    gold_standard = os.path.join(ref_dir, os.listdir(ref_dir)[0])\n",
    "    lang = gold_standard.split('/')[-1].replace('.tsv', '')\n",
    "    res_dir = os.path.join(input_dir, 'res')\n",
    "    submission_path = os.path.join(res_dir, os.listdir(res_dir)[0])\n",
    "    task = submission_path.split('/')[-1].replace('.tsv', '').split('_')[1]\n",
    "\n",
    "    output_file = open(os.path.join(output_dir, name + '.txt'), \"w\")\n",
    "    if task == 'a':\n",
    "        acc_hs, p_hs, r_hs, f1_hs = evaluate_a(submission_path, gold_standard)\n",
    "\n",
    "        # the scores for the leaderboard must be in a file named \"scores.txt\"\n",
    "        # https://github.com/codalab/codalab-competitions/wiki/User_Building-a-Scoring-Program-for-a-Competition#directory-structure-for-submissions\n",
    "\n",
    "        output_file.write(\"taskA_fscore: {0}\\n\".format(f1_hs))\n",
    "        output_file.write(\"taskA_precision: {0}\\n\".format(p_hs))\n",
    "        output_file.write(\"taskA_recall: {0}\\n\".format(r_hs))\n",
    "        output_file.write(\"taskA_accuracy: {0}\\n\".format(acc_hs))\n",
    "        print(\"taskA_fscore: {0}\".format(f1_hs))\n",
    "        print(\"taskA_precision: {0}\".format(p_hs))\n",
    "        print(\"taskA_recall: {0}\".format(r_hs))\n",
    "        print(\"taskA_accuracy: {0}\".format(acc_hs))\n",
    "    elif task == 'b':\n",
    "        macro_f1, emr, acc_levels, p_levels, r_levels, f1_levels = evaluate_b(submission_path, gold_standard)\n",
    "\n",
    "        # the scores for the leaderboard must be in a file named \"scores.txt\"\n",
    "        # https://github.com/codalab/codalab-competitions/wiki/User_Building-a-Scoring-Program-for-a-Competition#directory-structure-for-submissions\n",
    "\n",
    "        output_file.write(\"taskB_fscore_macro: {0}\\n\".format(macro_f1))\n",
    "        output_file.write(\"taskB_emr: {0}\\n\".format(emr))\n",
    "        output_file.write(\"taskB_fscore_HS: {0}\\n\".format(f1_levels[\"HS\"]))\n",
    "        output_file.write(\"taskB_precision_HS: {0}\\n\".format(p_levels[\"HS\"]))\n",
    "        output_file.write(\"taskB_recall_HS: {0}\\n\".format(r_levels[\"HS\"]))\n",
    "        output_file.write(\"taskB_accuracy_HS: {0}\\n\".format(acc_levels[\"HS\"]))\n",
    "        output_file.write(\"taskB_fscore_TR: {0}\\n\".format(f1_levels[\"TargetRange\"]))\n",
    "        output_file.write(\"taskB_precision_TR: {0}\\n\".format(p_levels[\"TargetRange\"]))\n",
    "        output_file.write(\"taskB_recall_TR: {0}\\n\".format(r_levels[\"TargetRange\"]))\n",
    "        output_file.write(\"taskB_accuracy_TR: {0}\\n\".format(acc_levels[\"TargetRange\"]))\n",
    "        output_file.write(\"taskB_fscore_AG: {0}\\n\".format(f1_levels[\"Aggressiveness\"]))\n",
    "        output_file.write(\"taskB_precision_AG: {0}\\n\".format(p_levels[\"Aggressiveness\"]))\n",
    "        output_file.write(\"taskB_recall_AG: {0}\\n\".format(r_levels[\"Aggressiveness\"]))\n",
    "        output_file.write(\"taskB_accuracy_AG: {0}\\n\".format(acc_levels[\"Aggressiveness\"]))\n",
    "\n",
    "        print(\"taskB_fscore_macro: {0}\".format(macro_f1))\n",
    "        print(\"taskB_emr: {0}n\".format(emr))\n",
    "        print(\"taskB_fscore_HS: {0}\".format(f1_levels[\"HS\"]))\n",
    "        print(\"taskB_precision_HS: {0}\".format(p_levels[\"HS\"]))\n",
    "        print(\"taskB_recall_HS: {0}\".format(r_levels[\"HS\"]))\n",
    "        print(\"taskB_accuracy_HS: {0}\".format(acc_levels[\"HS\"]))\n",
    "        print(\"taskB_fscore_TR: {0}\".format(f1_levels[\"TargetRange\"]))\n",
    "        print(\"taskB_precision_TR: {0}\".format(p_levels[\"TargetRange\"]))\n",
    "        print(\"taskB_recall_TR: {0}\".format(r_levels[\"TargetRange\"]))\n",
    "        print(\"taskB_accuracy_TR: {0}\".format(acc_levels[\"TargetRange\"]))\n",
    "        print(\"taskB_fscore_AG: {0}\".format(f1_levels[\"Aggressiveness\"]))\n",
    "        print(\"taskB_precision_AG: {0}\".format(p_levels[\"Aggressiveness\"]))\n",
    "        print(\"taskB_recall_AG: {0}\".format(r_levels[\"Aggressiveness\"]))\n",
    "        print(\"taskB_accuracy_AG: {0}\".format(acc_levels[\"Aggressiveness\"]))\n",
    "\n",
    "\n",
    "    output_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
