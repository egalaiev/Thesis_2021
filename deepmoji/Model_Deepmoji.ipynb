{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis 2020-2021: DeepMoji Model \n",
    "\n",
    "In this notebook, we will create a deepmoji model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "    \n",
    "df_train = pd.read_csv('data_csv/hateval2019_en_train.csv')#, encoding='cp1252')\n",
    "df_dev = pd.read_csv('data_csv/hateval2019_en_dev.csv')#, encoding='cp1252')\n",
    "\n",
    "df_train_dev = df_train.append(df_dev, ignore_index=True)\n",
    "df_train_dev = df_train_dev.drop(['TR', 'AG'], axis=1)\n",
    "\n",
    "df_test = pd.read_csv('data_csv/hateval2019_en_test.csv')\n",
    "df_test = df_test.drop(['TR', 'AG'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>Hurray, saving us $$$ in so many ways @potus @...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202</td>\n",
       "      <td>Why would young fighting age men be the vast m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>203</td>\n",
       "      <td>@KamalaHarris Illegals Dump their Kids at the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204</td>\n",
       "      <td>NY Times: 'Nearly All White' States Pose 'an A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205</td>\n",
       "      <td>Orban in Brussels: European leaders are ignori...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>19196</td>\n",
       "      <td>@SamEnvers you unfollowed me? Fuck you pussy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>19197</td>\n",
       "      <td>@DanReynolds STFU BITCH! AND YOU GO MAKE SOME ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>19198</td>\n",
       "      <td>@2beornotbeing Honey, as a fellow white chick,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>19199</td>\n",
       "      <td>I hate bitches who talk about niggaz with kids...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>19200</td>\n",
       "      <td>@AnnCoulter @DonaldJTrumpJr You won the\" life ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  HS\n",
       "0       201  Hurray, saving us $$$ in so many ways @potus @...   1\n",
       "1       202  Why would young fighting age men be the vast m...   1\n",
       "2       203  @KamalaHarris Illegals Dump their Kids at the ...   1\n",
       "3       204  NY Times: 'Nearly All White' States Pose 'an A...   0\n",
       "4       205  Orban in Brussels: European leaders are ignori...   0\n",
       "...     ...                                                ...  ..\n",
       "9995  19196       @SamEnvers you unfollowed me? Fuck you pussy   0\n",
       "9996  19197  @DanReynolds STFU BITCH! AND YOU GO MAKE SOME ...   1\n",
       "9997  19198  @2beornotbeing Honey, as a fellow white chick,...   0\n",
       "9998  19199  I hate bitches who talk about niggaz with kids...   1\n",
       "9999  19200  @AnnCoulter @DonaldJTrumpJr You won the\" life ...   1\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pattern.text.en import singularize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Create a function to clean the tweets\n",
    "def cleanTxt(text):\n",
    "    text = text.lower() # Convert everything to lower case\n",
    "    text = re.sub(r'@[a-zA-Z0-9]+', '', text) # Remove @mentions\n",
    "    text = re.sub(r'rt[\\s]+', '', text) # Remove RT (retweet symbol)\n",
    "    text = re.sub(r'&amp;', 'and', text) # Replace '&amp;' by 'and'\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text) # Remove hyper link  \n",
    "    #text = re.sub(r'\\d+', '0', text) # Replace all numbers by a zero\n",
    "    text = \" \".join([singularize(word) for word in tokenizer.tokenize(text) if word not in stop_words]) # Remove stopwords\n",
    "    #text = \" \".join([singularize(word) for word in text])\n",
    "    text = re.sub(r'[^\\w\\s#]', ' ', text) # Remove all non-alphanumeric symbols (excluding whitespace and # characters)\n",
    "    text = re.sub(r'\\s+', ' ', text) # Replace multiple whitespaces by a single whitespace\n",
    "    text = text.strip() # Remove whitespaces at the beginning and at the end\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2294</th>\n",
       "      <th>2295</th>\n",
       "      <th>2296</th>\n",
       "      <th>2297</th>\n",
       "      <th>2298</th>\n",
       "      <th>2299</th>\n",
       "      <th>2300</th>\n",
       "      <th>2301</th>\n",
       "      <th>2302</th>\n",
       "      <th>2303</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.063760</td>\n",
       "      <td>-0.002055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003050</td>\n",
       "      <td>-0.005823</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001246</td>\n",
       "      <td>0.100377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050156</td>\n",
       "      <td>-0.014659</td>\n",
       "      <td>-0.021302</td>\n",
       "      <td>-0.014753</td>\n",
       "      <td>0.072366</td>\n",
       "      <td>-0.036926</td>\n",
       "      <td>-0.024684</td>\n",
       "      <td>-0.004294</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.001240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.046231</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>-0.002589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.058986</td>\n",
       "      <td>-0.001589</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000862</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007312</td>\n",
       "      <td>0.043984</td>\n",
       "      <td>0.060464</td>\n",
       "      <td>-0.040505</td>\n",
       "      <td>0.009673</td>\n",
       "      <td>-0.027391</td>\n",
       "      <td>-0.006929</td>\n",
       "      <td>0.027512</td>\n",
       "      <td>-0.026444</td>\n",
       "      <td>0.008858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.010508</td>\n",
       "      <td>-0.058681</td>\n",
       "      <td>-0.004275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.152366</td>\n",
       "      <td>-0.003675</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>0.008206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.011392</td>\n",
       "      <td>0.007648</td>\n",
       "      <td>-0.014650</td>\n",
       "      <td>0.026785</td>\n",
       "      <td>-0.033066</td>\n",
       "      <td>-0.005839</td>\n",
       "      <td>0.015446</td>\n",
       "      <td>-0.013252</td>\n",
       "      <td>0.005346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.074244</td>\n",
       "      <td>-0.009066</td>\n",
       "      <td>-0.006454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.012025</td>\n",
       "      <td>-0.030088</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000315</td>\n",
       "      <td>0.004916</td>\n",
       "      <td>0.010824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035436</td>\n",
       "      <td>0.025413</td>\n",
       "      <td>0.032776</td>\n",
       "      <td>0.036215</td>\n",
       "      <td>0.005092</td>\n",
       "      <td>0.008667</td>\n",
       "      <td>-0.023761</td>\n",
       "      <td>-0.001856</td>\n",
       "      <td>0.009813</td>\n",
       "      <td>-0.026586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.028176</td>\n",
       "      <td>-0.045293</td>\n",
       "      <td>-0.002111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.091202</td>\n",
       "      <td>-0.004682</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000201</td>\n",
       "      <td>-0.008982</td>\n",
       "      <td>0.159621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052942</td>\n",
       "      <td>0.009692</td>\n",
       "      <td>0.016195</td>\n",
       "      <td>0.028719</td>\n",
       "      <td>0.073249</td>\n",
       "      <td>-0.058089</td>\n",
       "      <td>-0.037047</td>\n",
       "      <td>0.022545</td>\n",
       "      <td>-0.009876</td>\n",
       "      <td>-0.031878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2    3         4         5         6         7  \\\n",
       "0  0.000000 -0.063760 -0.002055  0.0  0.003050 -0.005823 -0.000113  0.000000   \n",
       "1 -0.046231  0.000309 -0.002589  0.0 -0.058986 -0.001589  0.001765 -0.000065   \n",
       "2 -0.010508 -0.058681 -0.004275  0.0 -0.152366 -0.003675  0.000355  0.000000   \n",
       "3 -0.074244 -0.009066 -0.006454  0.0 -0.012025 -0.030088 -0.000013 -0.000315   \n",
       "4 -0.028176 -0.045293 -0.002111  0.0 -0.091202 -0.004682  0.000009 -0.000201   \n",
       "\n",
       "          8         9  ...      2294      2295      2296      2297      2298  \\\n",
       "0 -0.001246  0.100377  ...  0.050156 -0.014659 -0.021302 -0.014753  0.072366   \n",
       "1 -0.000862  0.001762  ... -0.007312  0.043984  0.060464 -0.040505  0.009673   \n",
       "2  0.001535  0.008206  ...  0.007031  0.011392  0.007648 -0.014650  0.026785   \n",
       "3  0.004916  0.010824  ...  0.035436  0.025413  0.032776  0.036215  0.005092   \n",
       "4 -0.008982  0.159621  ...  0.052942  0.009692  0.016195  0.028719  0.073249   \n",
       "\n",
       "       2299      2300      2301      2302      2303  \n",
       "0 -0.036926 -0.024684 -0.004294  0.003646  0.001240  \n",
       "1 -0.027391 -0.006929  0.027512 -0.026444  0.008858  \n",
       "2 -0.033066 -0.005839  0.015446 -0.013252  0.005346  \n",
       "3  0.008667 -0.023761 -0.001856  0.009813 -0.026586  \n",
       "4 -0.058089 -0.037047  0.022545 -0.009876 -0.031878  \n",
       "\n",
       "[5 rows x 2304 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load (2304 long) deepmoji vectors (for first 5000 samples of training data) from csv file into variable encoding_part1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# write csv file\n",
    "#df = pd.DataFrame(encoding)\n",
    "#df.to_csv('encoding_part1.csv')\n",
    "\n",
    "# read csv file\n",
    "encoding_part1 = pd.read_csv(\"encoding_part1.csv\")\n",
    "del encoding_part1[\"Unnamed: 0\"]\n",
    "encoding_part1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2304)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_part1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing using dictionary from C:\\Users\\Admin\\Desktop\\Master thesis\\Thesis_2021\\deepmoji/model/vocabulary.json\n",
      "Loading model from C:\\Users\\Admin\\Desktop\\Master thesis\\Thesis_2021\\deepmoji/model/pytorch_model.bin.\n",
      "Loading weights for embed.weight\n",
      "Loading weights for lstm_0.weight_ih_l0\n",
      "Loading weights for lstm_0.weight_hh_l0\n",
      "Loading weights for lstm_0.bias_ih_l0\n",
      "Loading weights for lstm_0.bias_hh_l0\n",
      "Loading weights for lstm_0.weight_ih_l0_reverse\n",
      "Loading weights for lstm_0.weight_hh_l0_reverse\n",
      "Loading weights for lstm_0.bias_ih_l0_reverse\n",
      "Loading weights for lstm_0.bias_hh_l0_reverse\n",
      "Loading weights for lstm_1.weight_ih_l0\n",
      "Loading weights for lstm_1.weight_hh_l0\n",
      "Loading weights for lstm_1.bias_ih_l0\n",
      "Loading weights for lstm_1.bias_hh_l0\n",
      "Loading weights for lstm_1.weight_ih_l0_reverse\n",
      "Loading weights for lstm_1.weight_hh_l0_reverse\n",
      "Loading weights for lstm_1.bias_ih_l0_reverse\n",
      "Loading weights for lstm_1.bias_hh_l0_reverse\n",
      "Loading weights for attention_layer.attention_vector\n",
      "Ignoring weights for output_layer.0.weight\n",
      "Ignoring weights for output_layer.0.bias\n",
      "TorchMoji(\n",
      "  (embed): Embedding(50000, 256)\n",
      "  (embed_dropout): Dropout2d(p=0, inplace=False)\n",
      "  (lstm_0): LSTMHardSigmoid(256, 512, batch_first=True, bidirectional=True)\n",
      "  (lstm_1): LSTMHardSigmoid(1024, 512, batch_first=True, bidirectional=True)\n",
      "  (attention_layer): Attention(2304, return attention=False)\n",
      ")\n",
      "Encoding texts..\n"
     ]
    }
   ],
   "source": [
    "# Now fix (2304 long) deepmoji vectors for second 5000 samples of training data\n",
    "\n",
    "\"\"\" Use torchMoji to encode texts into emotional feature vectors.\n",
    "\"\"\"\n",
    "from __future__ import print_function, division, unicode_literals\n",
    "import json\n",
    "from torchmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from torchmoji.model_def import torchmoji_feature_encoding \n",
    "from torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n",
    "\n",
    "maxlen = 30\n",
    "batch_size = 32\n",
    "\n",
    "print('Tokenizing using dictionary from {}'.format(VOCAB_PATH))\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    vocabulary = json.load(f)\n",
    "st = SentenceTokenizer(vocabulary, maxlen)\n",
    "tokenized_second, _, _ = st.tokenize_sentences(df_train_dev.text[5000:])\n",
    "#print(\"TOKENIZED:\",tokenized)\n",
    "print('Loading model from {}.'.format(PRETRAINED_PATH))\n",
    "model = torchmoji_feature_encoding(PRETRAINED_PATH)\n",
    "print(model)\n",
    "\n",
    "print('Encoding texts..')\n",
    "encoding_second = model(tokenized_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2304)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_part1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (10000, 2304)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2294</th>\n",
       "      <th>2295</th>\n",
       "      <th>2296</th>\n",
       "      <th>2297</th>\n",
       "      <th>2298</th>\n",
       "      <th>2299</th>\n",
       "      <th>2300</th>\n",
       "      <th>2301</th>\n",
       "      <th>2302</th>\n",
       "      <th>2303</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.063760</td>\n",
       "      <td>-0.002055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003050</td>\n",
       "      <td>-0.005823</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001246</td>\n",
       "      <td>0.100377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050156</td>\n",
       "      <td>-0.014659</td>\n",
       "      <td>-0.021302</td>\n",
       "      <td>-0.014753</td>\n",
       "      <td>0.072366</td>\n",
       "      <td>-0.036926</td>\n",
       "      <td>-0.024684</td>\n",
       "      <td>-0.004294</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.001240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.046231</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>-0.002589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.058986</td>\n",
       "      <td>-0.001589</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000862</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007312</td>\n",
       "      <td>0.043984</td>\n",
       "      <td>0.060464</td>\n",
       "      <td>-0.040505</td>\n",
       "      <td>0.009673</td>\n",
       "      <td>-0.027391</td>\n",
       "      <td>-0.006929</td>\n",
       "      <td>0.027512</td>\n",
       "      <td>-0.026444</td>\n",
       "      <td>0.008858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.010508</td>\n",
       "      <td>-0.058681</td>\n",
       "      <td>-0.004275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.152366</td>\n",
       "      <td>-0.003675</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>0.008206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.011392</td>\n",
       "      <td>0.007648</td>\n",
       "      <td>-0.014650</td>\n",
       "      <td>0.026785</td>\n",
       "      <td>-0.033066</td>\n",
       "      <td>-0.005839</td>\n",
       "      <td>0.015446</td>\n",
       "      <td>-0.013252</td>\n",
       "      <td>0.005346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.074244</td>\n",
       "      <td>-0.009066</td>\n",
       "      <td>-0.006454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.012025</td>\n",
       "      <td>-0.030088</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000315</td>\n",
       "      <td>0.004916</td>\n",
       "      <td>0.010824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035436</td>\n",
       "      <td>0.025413</td>\n",
       "      <td>0.032776</td>\n",
       "      <td>0.036215</td>\n",
       "      <td>0.005092</td>\n",
       "      <td>0.008667</td>\n",
       "      <td>-0.023761</td>\n",
       "      <td>-0.001856</td>\n",
       "      <td>0.009813</td>\n",
       "      <td>-0.026586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.028176</td>\n",
       "      <td>-0.045293</td>\n",
       "      <td>-0.002111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.091202</td>\n",
       "      <td>-0.004682</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000201</td>\n",
       "      <td>-0.008982</td>\n",
       "      <td>0.159621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052942</td>\n",
       "      <td>0.009692</td>\n",
       "      <td>0.016195</td>\n",
       "      <td>0.028719</td>\n",
       "      <td>0.073249</td>\n",
       "      <td>-0.058089</td>\n",
       "      <td>-0.037047</td>\n",
       "      <td>0.022545</td>\n",
       "      <td>-0.009876</td>\n",
       "      <td>-0.031878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2    3         4         5         6         7  \\\n",
       "0  0.000000 -0.063760 -0.002055  0.0  0.003050 -0.005823 -0.000113  0.000000   \n",
       "1 -0.046231  0.000309 -0.002589  0.0 -0.058986 -0.001589  0.001765 -0.000065   \n",
       "2 -0.010508 -0.058681 -0.004275  0.0 -0.152366 -0.003675  0.000355  0.000000   \n",
       "3 -0.074244 -0.009066 -0.006454  0.0 -0.012025 -0.030088 -0.000013 -0.000315   \n",
       "4 -0.028176 -0.045293 -0.002111  0.0 -0.091202 -0.004682  0.000009 -0.000201   \n",
       "\n",
       "          8         9  ...      2294      2295      2296      2297      2298  \\\n",
       "0 -0.001246  0.100377  ...  0.050156 -0.014659 -0.021302 -0.014753  0.072366   \n",
       "1 -0.000862  0.001762  ... -0.007312  0.043984  0.060464 -0.040505  0.009673   \n",
       "2  0.001535  0.008206  ...  0.007031  0.011392  0.007648 -0.014650  0.026785   \n",
       "3  0.004916  0.010824  ...  0.035436  0.025413  0.032776  0.036215  0.005092   \n",
       "4 -0.008982  0.159621  ...  0.052942  0.009692  0.016195  0.028719  0.073249   \n",
       "\n",
       "       2299      2300      2301      2302      2303  \n",
       "0 -0.036926 -0.024684 -0.004294  0.003646  0.001240  \n",
       "1 -0.027391 -0.006929  0.027512 -0.026444  0.008858  \n",
       "2 -0.033066 -0.005839  0.015446 -0.013252  0.005346  \n",
       "3  0.008667 -0.023761 -0.001856  0.009813 -0.026586  \n",
       "4 -0.058089 -0.037047  0.022545 -0.009876 -0.031878  \n",
       "\n",
       "[5 rows x 2304 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_part2 = pd.DataFrame(encoding_second)\n",
    "encoding_part2.columns = encoding_part1.columns\n",
    "encoding_total = encoding_part1.append(encoding_part2)\n",
    "print(\"Size:\", encoding_total.shape)\n",
    "encoding_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for embed.weight\n",
      "Loading weights for lstm_0.weight_ih_l0\n",
      "Loading weights for lstm_0.weight_hh_l0\n",
      "Loading weights for lstm_0.bias_ih_l0\n",
      "Loading weights for lstm_0.bias_hh_l0\n",
      "Loading weights for lstm_0.weight_ih_l0_reverse\n",
      "Loading weights for lstm_0.weight_hh_l0_reverse\n",
      "Loading weights for lstm_0.bias_ih_l0_reverse\n",
      "Loading weights for lstm_0.bias_hh_l0_reverse\n",
      "Loading weights for lstm_1.weight_ih_l0\n",
      "Loading weights for lstm_1.weight_hh_l0\n",
      "Loading weights for lstm_1.bias_ih_l0\n",
      "Loading weights for lstm_1.bias_hh_l0\n",
      "Loading weights for lstm_1.weight_ih_l0_reverse\n",
      "Loading weights for lstm_1.weight_hh_l0_reverse\n",
      "Loading weights for lstm_1.bias_ih_l0_reverse\n",
      "Loading weights for lstm_1.bias_hh_l0_reverse\n",
      "Loading weights for attention_layer.attention_vector\n",
      "Ignoring weights for output_layer.0.weight\n",
      "Ignoring weights for output_layer.0.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Use torchMoji to encode test samples into emotional feature vectors.\n",
    "\"\"\"\n",
    "\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    vocabulary = json.load(f)\n",
    "st = SentenceTokenizer(vocabulary, maxlen)\n",
    "tokenized_test, _, _ = st.tokenize_sentences(df_test.text)\n",
    "model_test = torchmoji_feature_encoding(PRETRAINED_PATH)\n",
    "encoding_test = model_test(tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = encoding_total\n",
    "X_test = encoding_test\n",
    "y_train = df_train_dev.HS\n",
    "y_test = df_test.HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from evaluate.ipynb\n",
      "taskA_fscore: 0.526420692167044\n",
      "taskA_precision: 0.6063356553342925\n",
      "taskA_recall: 0.581992337164751\n",
      "taskA_accuracy: 0.54\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the DeepMoji model using Logistic Regression as the classifier (without normalizing input data)\n",
    "\n",
    "import import_ipynb\n",
    "import evaluate # here we import the local evaluate.ipynb jupyter notebook\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression().fit(X_train, y_train)\n",
    "y_predict = logreg.predict(X_test)\n",
    "\n",
    "df_test_deepm = df_test.copy()\n",
    "df_test_deepm['HS'] = y_predict\n",
    "\n",
    "# Create prediction file for the deepmoji_baseline\n",
    "df_test_deepm[['id', 'HS']].to_csv('predictions/deepmoji_baseline.tsv', sep='\\t', index=False, header=False)\n",
    "df_test_deepm[['id', 'HS']].to_csv('input/res/en_a.tsv', sep='\\t', index=False, header=False)\n",
    "\n",
    "# Evaluate the result of the deepmoji_baseline\n",
    "evaluate.write_eval(\"scores_deepmoji_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taskA_fscore: 0.5411640211640212\n",
      "taskA_precision: 0.597527747726089\n",
      "taskA_recall: 0.583264915161467\n",
      "taskA_accuracy: 0.5483333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Normalize the data via StandardScaler\n",
    "\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Evaluate the DeepMoji model using Logistic Regression as the classifier (+ normalizing input data)\n",
    "\n",
    "logreg_scaled = LogisticRegression().fit(X_train_scaled, y_train)\n",
    "y_predict_scaled = logreg_scaled.predict(X_test_scaled)\n",
    "\n",
    "df_test_deepmoji_scaled = df_test.copy()\n",
    "df_test_deepmoji_scaled['HS'] = y_predict_scaled\n",
    "\n",
    "# Create prediction file for the deepmoji_scaled\n",
    "df_test_deepmoji_scaled[['id', 'HS']].to_csv('predictions/deepmoji_scaled.tsv', sep='\\t', index=False, header=False)\n",
    "df_test_deepmoji_scaled[['id', 'HS']].to_csv('input/res/en_a.tsv', sep='\\t', index=False, header=False)\n",
    "\n",
    "# Evaluate the result of the deepmoji_scaled\n",
    "evaluate.write_eval(\"scores_deepmoji_scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=5)]: Done  62 tasks      | elapsed:   57.8s\n",
      "[Parallel(n_jobs=5)]: Done 152 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=5)]: Done 270 out of 270 | elapsed: 10.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score:  0.693147302500796\n",
      "Best parameters:  {'C': 0.001, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "f1 = make_scorer(f1_score , average='macro')\n",
    "params = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'solver': ('newton-cg', 'lbfgs', 'liblinear'), 'penalty': ('l1', 'l2', 'elasticnet')}\n",
    "grid = GridSearchCV(estimator=LogisticRegression(max_iter=500), param_grid=params, cv=5, scoring=f1, verbose=5, n_jobs=5)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "print(\"Best cross-validation score: \", grid.best_score_)\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taskA_fscore: 0.545005086132736\n",
      "taskA_precision: 0.5999845737177499\n",
      "taskA_recall: 0.5860290093048715\n",
      "taskA_accuracy: 0.5516666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Evaluate the DeepMoji model using optimized Logistic Regression as the classifier (+ normalizing input data)\n",
    "\n",
    "logreg_optimized = LogisticRegression(solver='liblinear').fit(X_train_scaled, y_train)\n",
    "y_predict_optimized = logreg_optimized.predict(X_test_scaled)\n",
    "\n",
    "df_test_deepmoji_optimized = df_test.copy()\n",
    "df_test_deepmoji_optimized['HS'] = y_predict_optimized\n",
    "\n",
    "# Create prediction file for the deepmoji_optimized\n",
    "df_test_deepmoji_optimized[['id', 'HS']].to_csv('predictions/deepmoji_optimized.tsv', sep='\\t', index=False, header=False)\n",
    "df_test_deepmoji_optimized[['id', 'HS']].to_csv('input/res/en_a.tsv', sep='\\t', index=False, header=False)\n",
    "\n",
    "# Evaluate the result of the deepmoji_optimized\n",
    "evaluate.write_eval(\"scores_deepmoji_optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taskA_fscore: 0.5368626086956522\n",
      "taskA_precision: 0.5440589688278884\n",
      "taskA_recall: 0.5449507389162562\n",
      "taskA_accuracy: 0.5376666666666666\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using Multinomial Naive Bayes as the classifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mm_scaler = MinMaxScaler()\n",
    "X_train_nb = mm_scaler.fit_transform(X_train_scaled)\n",
    "X_test_nb = mm_scaler.transform(X_test_scaled)\n",
    "\n",
    "nb = MultinomialNB().fit(X_train_nb, y_train)\n",
    "y_predict_nb = nb.predict(X_test_nb)\n",
    "\n",
    "# Create new test dataframe\n",
    "df_test_deepmoji_nb = df_test.copy()\n",
    "df_test_deepmoji_nb['HS'] = y_predict_nb\n",
    "\n",
    "# Create prediction file for the deepmoji_nb\n",
    "df_test_deepmoji_nb[['id', 'HS']].to_csv('predictions/deepmoji_nb.tsv', sep='\\t', index=False, header=False)\n",
    "df_test_deepmoji_nb[['id', 'HS']].to_csv('input/res/en_a.tsv', sep='\\t', index=False, header=False)\n",
    "\n",
    "# Evaluate the result of the deepmoji_nb\n",
    "evaluate.write_eval(\"scores_deepmoji_nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taskA_fscore: 0.5220857965880091\n",
      "taskA_precision: 0.5917577968986676\n",
      "taskA_recall: 0.5732484948002189\n",
      "taskA_accuracy: 0.5336666666666666\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using Random Forest as the classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier().fit(X_train_scaled, y_train)\n",
    "y_predict_rf = rf.predict(X_test_scaled)\n",
    "\n",
    "# Create new test dataframe\n",
    "df_test_deepmoji_rf = df_test.copy()\n",
    "df_test_deepmoji_rf['HS'] = y_predict_rf\n",
    "\n",
    "# Create prediction file for the deepmoji_rf\n",
    "df_test_deepmoji_rf[['id', 'HS']].to_csv('predictions/deepmoji_rf.tsv', sep='\\t', index=False, header=False)\n",
    "df_test_deepmoji_rf[['id', 'HS']].to_csv('input/res/en_a.tsv', sep='\\t', index=False, header=False)\n",
    "\n",
    "# Evaluate the result of the deepmoji_rf\n",
    "evaluate.write_eval(\"scores_deepmoji_rf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will repeat the same things, but we will clean the data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text\n",
    "df_train_dev['text_cleaned'] = df_train_dev['text'].apply(cleanTxt)\n",
    "df_test['text_cleaned'] = df_test['text'].apply(cleanTxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>Hurray, saving us $$$ in so many ways @potus @...</td>\n",
       "      <td>1</td>\n",
       "      <td>hurray saving u many way #lockthemup #buildthe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202</td>\n",
       "      <td>Why would young fighting age men be the vast m...</td>\n",
       "      <td>1</td>\n",
       "      <td>would young fighting age man vast majority one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>203</td>\n",
       "      <td>@KamalaHarris Illegals Dump their Kids at the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>illegal dump kid border like road kill refuse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204</td>\n",
       "      <td>NY Times: 'Nearly All White' States Pose 'an A...</td>\n",
       "      <td>0</td>\n",
       "      <td>ny time s nearly white s state pose s array pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205</td>\n",
       "      <td>Orban in Brussels: European leaders are ignori...</td>\n",
       "      <td>0</td>\n",
       "      <td>orban brussel european leader ignoring person ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>19196</td>\n",
       "      <td>@SamEnvers you unfollowed me? Fuck you pussy</td>\n",
       "      <td>0</td>\n",
       "      <td>unfollowed fuck pussy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>19197</td>\n",
       "      <td>@DanReynolds STFU BITCH! AND YOU GO MAKE SOME ...</td>\n",
       "      <td>1</td>\n",
       "      <td>stfu bitch go make satanic music u illuminatus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>19198</td>\n",
       "      <td>@2beornotbeing Honey, as a fellow white chick,...</td>\n",
       "      <td>0</td>\n",
       "      <td>honey fellow white chick let tell need shut fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>19199</td>\n",
       "      <td>I hate bitches who talk about niggaz with kids...</td>\n",
       "      <td>1</td>\n",
       "      <td>hate bitch talk niggaz kid everybody cant find...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>19200</td>\n",
       "      <td>@AnnCoulter @DonaldJTrumpJr You won the\" life ...</td>\n",
       "      <td>1</td>\n",
       "      <td>life time recipient hysterical woman long time...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  HS  \\\n",
       "0       201  Hurray, saving us $$$ in so many ways @potus @...   1   \n",
       "1       202  Why would young fighting age men be the vast m...   1   \n",
       "2       203  @KamalaHarris Illegals Dump their Kids at the ...   1   \n",
       "3       204  NY Times: 'Nearly All White' States Pose 'an A...   0   \n",
       "4       205  Orban in Brussels: European leaders are ignori...   0   \n",
       "...     ...                                                ...  ..   \n",
       "9995  19196       @SamEnvers you unfollowed me? Fuck you pussy   0   \n",
       "9996  19197  @DanReynolds STFU BITCH! AND YOU GO MAKE SOME ...   1   \n",
       "9997  19198  @2beornotbeing Honey, as a fellow white chick,...   0   \n",
       "9998  19199  I hate bitches who talk about niggaz with kids...   1   \n",
       "9999  19200  @AnnCoulter @DonaldJTrumpJr You won the\" life ...   1   \n",
       "\n",
       "                                           text_cleaned  \n",
       "0     hurray saving u many way #lockthemup #buildthe...  \n",
       "1     would young fighting age man vast majority one...  \n",
       "2     illegal dump kid border like road kill refuse ...  \n",
       "3     ny time s nearly white s state pose s array pr...  \n",
       "4     orban brussel european leader ignoring person ...  \n",
       "...                                                 ...  \n",
       "9995                              unfollowed fuck pussy  \n",
       "9996  stfu bitch go make satanic music u illuminatus...  \n",
       "9997  honey fellow white chick let tell need shut fu...  \n",
       "9998  hate bitch talk niggaz kid everybody cant find...  \n",
       "9999  life time recipient hysterical woman long time...  \n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
