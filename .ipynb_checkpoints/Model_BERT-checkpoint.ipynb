{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis 2020-2021: BERT\n",
    "\n",
    "In this notebook, we will create a BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pattern.text.en import singularize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Create a function to clean the tweets\n",
    "def cleanTxt(text):\n",
    "    text = text.lower() # Convert everything to lower case\n",
    "    text = re.sub(r'@[a-zA-Z0-9]+', '', text) # Remove @mentions\n",
    "    text = re.sub(r'rt[\\s]+', '', text) # Remove RT (retweet symbol)\n",
    "    text = re.sub(r'&amp;', 'and', text) # Replace '&amp;' by 'and'\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text) # Remove hyper link  \n",
    "    #text = re.sub(r'\\d+', '0', text) # Replace all numbers by a zero\n",
    "    text = \" \".join([singularize(word) for word in tweet_tokenizer.tokenize(text) if word not in stop_words]) # Remove stopwords\n",
    "    #text = \" \".join([singularize(word) for word in text])\n",
    "    #text = re.sub(r'[^\\w\\s#]', ' ', text) # Remove all non-alphanumeric symbols (excluding whitespace and # characters)\n",
    "    text = re.sub(r'\\s+', ' ', text) # Replace multiple whitespaces by a single whitespace\n",
    "    text = text.strip() # Remove whitespaces at the beginning and at the end\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>Hurray, saving us $$$ in so many ways @potus @...</td>\n",
       "      <td>1</td>\n",
       "      <td>hurray , saving u $ $ $ many way #lockthemup #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202</td>\n",
       "      <td>Why would young fighting age men be the vast m...</td>\n",
       "      <td>1</td>\n",
       "      <td>would young fighting age man vast majority one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>203</td>\n",
       "      <td>@KamalaHarris Illegals Dump their Kids at the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>illegal dump kid border like road kill refuse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204</td>\n",
       "      <td>NY Times: 'Nearly All White' States Pose 'an A...</td>\n",
       "      <td>0</td>\n",
       "      <td>ny time : 's nearly white 's state pose 's arr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205</td>\n",
       "      <td>Orban in Brussels: European leaders are ignori...</td>\n",
       "      <td>0</td>\n",
       "      <td>orban brussel : european leader ignoring perso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>19196</td>\n",
       "      <td>@SamEnvers you unfollowed me? Fuck you pussy</td>\n",
       "      <td>0</td>\n",
       "      <td>unfollowed ? fuck pussy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>19197</td>\n",
       "      <td>@DanReynolds STFU BITCH! AND YOU GO MAKE SOME ...</td>\n",
       "      <td>1</td>\n",
       "      <td>stfu bitch ! go make satanic music u illuminat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>19198</td>\n",
       "      <td>@2beornotbeing Honey, as a fellow white chick,...</td>\n",
       "      <td>0</td>\n",
       "      <td>honey , fellow white chick , let tell need . s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>19199</td>\n",
       "      <td>I hate bitches who talk about niggaz with kids...</td>\n",
       "      <td>1</td>\n",
       "      <td>hate bitch talk niggaz kid , everybody cant fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>19200</td>\n",
       "      <td>@AnnCoulter @DonaldJTrumpJr You won the\" life ...</td>\n",
       "      <td>1</td>\n",
       "      <td>\" life time recipient hysterical woman \" long ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  HS  \\\n",
       "0       201  Hurray, saving us $$$ in so many ways @potus @...   1   \n",
       "1       202  Why would young fighting age men be the vast m...   1   \n",
       "2       203  @KamalaHarris Illegals Dump their Kids at the ...   1   \n",
       "3       204  NY Times: 'Nearly All White' States Pose 'an A...   0   \n",
       "4       205  Orban in Brussels: European leaders are ignori...   0   \n",
       "...     ...                                                ...  ..   \n",
       "9995  19196       @SamEnvers you unfollowed me? Fuck you pussy   0   \n",
       "9996  19197  @DanReynolds STFU BITCH! AND YOU GO MAKE SOME ...   1   \n",
       "9997  19198  @2beornotbeing Honey, as a fellow white chick,...   0   \n",
       "9998  19199  I hate bitches who talk about niggaz with kids...   1   \n",
       "9999  19200  @AnnCoulter @DonaldJTrumpJr You won the\" life ...   1   \n",
       "\n",
       "                                           text_cleaned  \n",
       "0     hurray , saving u $ $ $ many way #lockthemup #...  \n",
       "1     would young fighting age man vast majority one...  \n",
       "2     illegal dump kid border like road kill refuse ...  \n",
       "3     ny time : 's nearly white 's state pose 's arr...  \n",
       "4     orban brussel : european leader ignoring perso...  \n",
       "...                                                 ...  \n",
       "9995                            unfollowed ? fuck pussy  \n",
       "9996  stfu bitch ! go make satanic music u illuminat...  \n",
       "9997  honey , fellow white chick , let tell need . s...  \n",
       "9998  hate bitch talk niggaz kid , everybody cant fi...  \n",
       "9999  \" life time recipient hysterical woman \" long ...  \n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "    \n",
    "df_train = pd.read_csv('data/hateval2019_en_train.csv')\n",
    "df_dev = pd.read_csv('data/hateval2019_en_dev.csv')\n",
    "\n",
    "df_train_dev = df_train.append(df_dev, ignore_index=True)\n",
    "df_train_dev = df_train_dev.drop(['TR', 'AG'], axis=1)\n",
    "\n",
    "df_test = pd.read_csv('data/hateval2019_en_test.csv')\n",
    "df_test = df_test.drop(['TR', 'AG'], axis=1)\n",
    "\n",
    "# Clean the data\n",
    "\n",
    "df_train_dev['text_cleaned'] = df_train_dev['text'].apply(cleanTxt)\n",
    "df_test['text_cleaned'] = df_test['text'].apply(cleanTxt)\n",
    "df_train_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.0.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_bert</th>\n",
       "      <th>text_cleaned_bert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>Hurray, saving us $$$ in so many ways @potus @...</td>\n",
       "      <td>1</td>\n",
       "      <td>hurray , saving u $ $ $ many way #lockthemup #...</td>\n",
       "      <td>[[CLS], hu, ##rra, ##y, ,, saving, us, $, $, $...</td>\n",
       "      <td>[[CLS], hu, ##rra, ##y, ,, saving, u, $, $, $,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202</td>\n",
       "      <td>Why would young fighting age men be the vast m...</td>\n",
       "      <td>1</td>\n",
       "      <td>would young fighting age man vast majority one...</td>\n",
       "      <td>[[CLS], why, would, young, fighting, age, men,...</td>\n",
       "      <td>[[CLS], would, young, fighting, age, man, vast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>203</td>\n",
       "      <td>@KamalaHarris Illegals Dump their Kids at the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>illegal dump kid border like road kill refuse ...</td>\n",
       "      <td>[[CLS], @, kamal, ##aha, ##rri, ##s, illegal, ...</td>\n",
       "      <td>[[CLS], illegal, dump, kid, border, like, road...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204</td>\n",
       "      <td>NY Times: 'Nearly All White' States Pose 'an A...</td>\n",
       "      <td>0</td>\n",
       "      <td>ny time : 's nearly white 's state pose 's arr...</td>\n",
       "      <td>[[CLS], ny, times, :, ', nearly, all, white, '...</td>\n",
       "      <td>[[CLS], ny, time, :, ', s, nearly, white, ', s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205</td>\n",
       "      <td>Orban in Brussels: European leaders are ignori...</td>\n",
       "      <td>0</td>\n",
       "      <td>orban brussel : european leader ignoring perso...</td>\n",
       "      <td>[[CLS], orb, ##an, in, brussels, :, european, ...</td>\n",
       "      <td>[[CLS], orb, ##an, br, ##uss, ##el, :, europea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>19196</td>\n",
       "      <td>@SamEnvers you unfollowed me? Fuck you pussy</td>\n",
       "      <td>0</td>\n",
       "      <td>unfollowed ? fuck pussy</td>\n",
       "      <td>[[CLS], @, same, ##n, ##vers, you, un, ##fo, #...</td>\n",
       "      <td>[[CLS], un, ##fo, ##llo, ##wed, ?, fuck, pussy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>19197</td>\n",
       "      <td>@DanReynolds STFU BITCH! AND YOU GO MAKE SOME ...</td>\n",
       "      <td>1</td>\n",
       "      <td>stfu bitch ! go make satanic music u illuminat...</td>\n",
       "      <td>[[CLS], @, dan, ##rey, ##no, ##ld, ##s, st, ##...</td>\n",
       "      <td>[[CLS], st, ##fu, bitch, !, go, make, satan, #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>19198</td>\n",
       "      <td>@2beornotbeing Honey, as a fellow white chick,...</td>\n",
       "      <td>0</td>\n",
       "      <td>honey , fellow white chick , let tell need . s...</td>\n",
       "      <td>[[CLS], @, 2, ##be, ##orno, ##t, ##bei, ##ng, ...</td>\n",
       "      <td>[[CLS], honey, ,, fellow, white, chick, ,, let...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>19199</td>\n",
       "      <td>I hate bitches who talk about niggaz with kids...</td>\n",
       "      <td>1</td>\n",
       "      <td>hate bitch talk niggaz kid , everybody cant fi...</td>\n",
       "      <td>[[CLS], i, hate, bitch, ##es, who, talk, about...</td>\n",
       "      <td>[[CLS], hate, bitch, talk, ni, ##gga, ##z, kid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>19200</td>\n",
       "      <td>@AnnCoulter @DonaldJTrumpJr You won the\" life ...</td>\n",
       "      <td>1</td>\n",
       "      <td>\" life time recipient hysterical woman \" long ...</td>\n",
       "      <td>[[CLS], @, ann, ##co, ##ult, ##er, @, donald, ...</td>\n",
       "      <td>[[CLS], \", life, time, recipient, hysterical, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  HS  \\\n",
       "0       201  Hurray, saving us $$$ in so many ways @potus @...   1   \n",
       "1       202  Why would young fighting age men be the vast m...   1   \n",
       "2       203  @KamalaHarris Illegals Dump their Kids at the ...   1   \n",
       "3       204  NY Times: 'Nearly All White' States Pose 'an A...   0   \n",
       "4       205  Orban in Brussels: European leaders are ignori...   0   \n",
       "...     ...                                                ...  ..   \n",
       "9995  19196       @SamEnvers you unfollowed me? Fuck you pussy   0   \n",
       "9996  19197  @DanReynolds STFU BITCH! AND YOU GO MAKE SOME ...   1   \n",
       "9997  19198  @2beornotbeing Honey, as a fellow white chick,...   0   \n",
       "9998  19199  I hate bitches who talk about niggaz with kids...   1   \n",
       "9999  19200  @AnnCoulter @DonaldJTrumpJr You won the\" life ...   1   \n",
       "\n",
       "                                           text_cleaned  \\\n",
       "0     hurray , saving u $ $ $ many way #lockthemup #...   \n",
       "1     would young fighting age man vast majority one...   \n",
       "2     illegal dump kid border like road kill refuse ...   \n",
       "3     ny time : 's nearly white 's state pose 's arr...   \n",
       "4     orban brussel : european leader ignoring perso...   \n",
       "...                                                 ...   \n",
       "9995                            unfollowed ? fuck pussy   \n",
       "9996  stfu bitch ! go make satanic music u illuminat...   \n",
       "9997  honey , fellow white chick , let tell need . s...   \n",
       "9998  hate bitch talk niggaz kid , everybody cant fi...   \n",
       "9999  \" life time recipient hysterical woman \" long ...   \n",
       "\n",
       "                                              text_bert  \\\n",
       "0     [[CLS], hu, ##rra, ##y, ,, saving, us, $, $, $...   \n",
       "1     [[CLS], why, would, young, fighting, age, men,...   \n",
       "2     [[CLS], @, kamal, ##aha, ##rri, ##s, illegal, ...   \n",
       "3     [[CLS], ny, times, :, ', nearly, all, white, '...   \n",
       "4     [[CLS], orb, ##an, in, brussels, :, european, ...   \n",
       "...                                                 ...   \n",
       "9995  [[CLS], @, same, ##n, ##vers, you, un, ##fo, #...   \n",
       "9996  [[CLS], @, dan, ##rey, ##no, ##ld, ##s, st, ##...   \n",
       "9997  [[CLS], @, 2, ##be, ##orno, ##t, ##bei, ##ng, ...   \n",
       "9998  [[CLS], i, hate, bitch, ##es, who, talk, about...   \n",
       "9999  [[CLS], @, ann, ##co, ##ult, ##er, @, donald, ...   \n",
       "\n",
       "                                      text_cleaned_bert  \n",
       "0     [[CLS], hu, ##rra, ##y, ,, saving, u, $, $, $,...  \n",
       "1     [[CLS], would, young, fighting, age, man, vast...  \n",
       "2     [[CLS], illegal, dump, kid, border, like, road...  \n",
       "3     [[CLS], ny, time, :, ', s, nearly, white, ', s...  \n",
       "4     [[CLS], orb, ##an, br, ##uss, ##el, :, europea...  \n",
       "...                                                 ...  \n",
       "9995  [[CLS], un, ##fo, ##llo, ##wed, ?, fuck, pussy...  \n",
       "9996  [[CLS], st, ##fu, bitch, !, go, make, satan, #...  \n",
       "9997  [[CLS], honey, ,, fellow, white, chick, ,, let...  \n",
       "9998  [[CLS], hate, bitch, talk, ni, ##gga, ##z, kid...  \n",
       "9999  [[CLS], \", life, time, recipient, hysterical, ...  \n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    return \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "df_train_dev['text_bert'] = df_train_dev['text'].apply(preprocess)\n",
    "df_train_dev['text_cleaned_bert'] = df_train_dev['text_cleaned'].apply(preprocess)\n",
    "df_test['text_bert'] = df_test['text'].apply(preprocess)\n",
    "df_test['text_cleaned_bert'] = df_test['text_cleaned'].apply(preprocess)\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "df_train_dev['text_bert'] = df_train_dev['text_bert'].apply(lambda x: tokenizer.tokenize(x))\n",
    "df_train_dev['text_cleaned_bert'] = df_train_dev['text_cleaned_bert'].apply(lambda x: tokenizer.tokenize(x))\n",
    "df_test['text_bert'] = df_test['text_bert'].apply(lambda x: tokenizer.tokenize(x))\n",
    "df_test['text_cleaned_bert'] = df_test['text_cleaned_bert'].apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "#tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "df_train_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_bert</th>\n",
       "      <th>text_cleaned_bert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>Hurray, saving us $$$ in so many ways @potus @...</td>\n",
       "      <td>1</td>\n",
       "      <td>hurray , saving u $ $ $ many way #lockthemup #...</td>\n",
       "      <td>[101, 15876, 11335, 2100, 1010, 7494, 2149, 10...</td>\n",
       "      <td>[101, 15876, 11335, 2100, 1010, 7494, 1057, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202</td>\n",
       "      <td>Why would young fighting age men be the vast m...</td>\n",
       "      <td>1</td>\n",
       "      <td>would young fighting age man vast majority one...</td>\n",
       "      <td>[101, 2339, 2052, 2402, 3554, 2287, 2273, 2022...</td>\n",
       "      <td>[101, 2052, 2402, 3554, 2287, 2158, 6565, 3484...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>203</td>\n",
       "      <td>@KamalaHarris Illegals Dump their Kids at the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>illegal dump kid border like road kill refuse ...</td>\n",
       "      <td>[101, 1030, 21911, 23278, 18752, 2015, 6206, 2...</td>\n",
       "      <td>[101, 6206, 15653, 4845, 3675, 2066, 2346, 310...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204</td>\n",
       "      <td>NY Times: 'Nearly All White' States Pose 'an A...</td>\n",
       "      <td>0</td>\n",
       "      <td>ny time : 's nearly white 's state pose 's arr...</td>\n",
       "      <td>[101, 6396, 2335, 1024, 1005, 3053, 2035, 2317...</td>\n",
       "      <td>[101, 6396, 2051, 1024, 1005, 1055, 3053, 2317...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205</td>\n",
       "      <td>Orban in Brussels: European leaders are ignori...</td>\n",
       "      <td>0</td>\n",
       "      <td>orban brussel : european leader ignoring perso...</td>\n",
       "      <td>[101, 19607, 2319, 1999, 9371, 1024, 2647, 417...</td>\n",
       "      <td>[101, 19607, 2319, 7987, 17854, 2884, 1024, 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>19196</td>\n",
       "      <td>@SamEnvers you unfollowed me? Fuck you pussy</td>\n",
       "      <td>0</td>\n",
       "      <td>unfollowed ? fuck pussy</td>\n",
       "      <td>[101, 1030, 2168, 2078, 14028, 2017, 4895, 148...</td>\n",
       "      <td>[101, 4895, 14876, 7174, 15557, 1029, 6616, 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>19197</td>\n",
       "      <td>@DanReynolds STFU BITCH! AND YOU GO MAKE SOME ...</td>\n",
       "      <td>1</td>\n",
       "      <td>stfu bitch ! go make satanic music u illuminat...</td>\n",
       "      <td>[101, 1030, 4907, 15202, 3630, 6392, 2015, 235...</td>\n",
       "      <td>[101, 2358, 11263, 7743, 999, 2175, 2191, 1679...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>19198</td>\n",
       "      <td>@2beornotbeing Honey, as a fellow white chick,...</td>\n",
       "      <td>0</td>\n",
       "      <td>honey , fellow white chick , let tell need . s...</td>\n",
       "      <td>[101, 1030, 1016, 4783, 26295, 2102, 19205, 30...</td>\n",
       "      <td>[101, 6861, 1010, 3507, 2317, 14556, 1010, 229...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>19199</td>\n",
       "      <td>I hate bitches who talk about niggaz with kids...</td>\n",
       "      <td>1</td>\n",
       "      <td>hate bitch talk niggaz kid , everybody cant fi...</td>\n",
       "      <td>[101, 1045, 5223, 7743, 2229, 2040, 2831, 2055...</td>\n",
       "      <td>[101, 5223, 7743, 2831, 9152, 23033, 2480, 484...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>19200</td>\n",
       "      <td>@AnnCoulter @DonaldJTrumpJr You won the\" life ...</td>\n",
       "      <td>1</td>\n",
       "      <td>\" life time recipient hysterical woman \" long ...</td>\n",
       "      <td>[101, 1030, 5754, 3597, 11314, 2121, 1030, 622...</td>\n",
       "      <td>[101, 1000, 2166, 2051, 7799, 25614, 2450, 100...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  HS  \\\n",
       "0       201  Hurray, saving us $$$ in so many ways @potus @...   1   \n",
       "1       202  Why would young fighting age men be the vast m...   1   \n",
       "2       203  @KamalaHarris Illegals Dump their Kids at the ...   1   \n",
       "3       204  NY Times: 'Nearly All White' States Pose 'an A...   0   \n",
       "4       205  Orban in Brussels: European leaders are ignori...   0   \n",
       "...     ...                                                ...  ..   \n",
       "9995  19196       @SamEnvers you unfollowed me? Fuck you pussy   0   \n",
       "9996  19197  @DanReynolds STFU BITCH! AND YOU GO MAKE SOME ...   1   \n",
       "9997  19198  @2beornotbeing Honey, as a fellow white chick,...   0   \n",
       "9998  19199  I hate bitches who talk about niggaz with kids...   1   \n",
       "9999  19200  @AnnCoulter @DonaldJTrumpJr You won the\" life ...   1   \n",
       "\n",
       "                                           text_cleaned  \\\n",
       "0     hurray , saving u $ $ $ many way #lockthemup #...   \n",
       "1     would young fighting age man vast majority one...   \n",
       "2     illegal dump kid border like road kill refuse ...   \n",
       "3     ny time : 's nearly white 's state pose 's arr...   \n",
       "4     orban brussel : european leader ignoring perso...   \n",
       "...                                                 ...   \n",
       "9995                            unfollowed ? fuck pussy   \n",
       "9996  stfu bitch ! go make satanic music u illuminat...   \n",
       "9997  honey , fellow white chick , let tell need . s...   \n",
       "9998  hate bitch talk niggaz kid , everybody cant fi...   \n",
       "9999  \" life time recipient hysterical woman \" long ...   \n",
       "\n",
       "                                              text_bert  \\\n",
       "0     [101, 15876, 11335, 2100, 1010, 7494, 2149, 10...   \n",
       "1     [101, 2339, 2052, 2402, 3554, 2287, 2273, 2022...   \n",
       "2     [101, 1030, 21911, 23278, 18752, 2015, 6206, 2...   \n",
       "3     [101, 6396, 2335, 1024, 1005, 3053, 2035, 2317...   \n",
       "4     [101, 19607, 2319, 1999, 9371, 1024, 2647, 417...   \n",
       "...                                                 ...   \n",
       "9995  [101, 1030, 2168, 2078, 14028, 2017, 4895, 148...   \n",
       "9996  [101, 1030, 4907, 15202, 3630, 6392, 2015, 235...   \n",
       "9997  [101, 1030, 1016, 4783, 26295, 2102, 19205, 30...   \n",
       "9998  [101, 1045, 5223, 7743, 2229, 2040, 2831, 2055...   \n",
       "9999  [101, 1030, 5754, 3597, 11314, 2121, 1030, 622...   \n",
       "\n",
       "                                      text_cleaned_bert  \n",
       "0     [101, 15876, 11335, 2100, 1010, 7494, 1057, 10...  \n",
       "1     [101, 2052, 2402, 3554, 2287, 2158, 6565, 3484...  \n",
       "2     [101, 6206, 15653, 4845, 3675, 2066, 2346, 310...  \n",
       "3     [101, 6396, 2051, 1024, 1005, 1055, 3053, 2317...  \n",
       "4     [101, 19607, 2319, 7987, 17854, 2884, 1024, 26...  \n",
       "...                                                 ...  \n",
       "9995  [101, 4895, 14876, 7174, 15557, 1029, 6616, 22...  \n",
       "9996  [101, 2358, 11263, 7743, 999, 2175, 2191, 1679...  \n",
       "9997  [101, 6861, 1010, 3507, 2317, 14556, 1010, 229...  \n",
       "9998  [101, 5223, 7743, 2831, 9152, 23033, 2480, 484...  \n",
       "9999  [101, 1000, 2166, 2051, 7799, 25614, 2450, 100...  \n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map the token strings to their vocabulary indeces.\n",
    "df_train_dev['text_bert'] = df_train_dev['text_bert'].apply(lambda x: tokenizer.convert_tokens_to_ids(x))\n",
    "df_train_dev['text_cleaned_bert'] = df_train_dev['text_cleaned_bert'].apply(lambda x: tokenizer.convert_tokens_to_ids(x))\n",
    "df_test['text_bert'] = df_test['text_bert'].apply(lambda x: tokenizer.convert_tokens_to_ids(x))\n",
    "df_test['text_cleaned_bert'] = df_test['text_cleaned_bert'].apply(lambda x: tokenizer.convert_tokens_to_ids(x))\n",
    "\n",
    "df_train_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 2453532433136 acquired on C:\\Users\\Admin/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6e4f2ff30c4ae1ac3df960a272ab71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 2453532433136 released on C:\\Users\\Admin/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 2453517282752 acquired on C:\\Users\\Admin/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ebe065d8c5741fd9be3570024a1c490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=440473133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 2453517282752 released on C:\\Users\\Admin/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [101, 15876, 11335, 2100, 1010, 7494, 2149, 10...\n",
       "1       [101, 2339, 2052, 2402, 3554, 2287, 2273, 2022...\n",
       "2       [101, 1030, 21911, 23278, 18752, 2015, 6206, 2...\n",
       "3       [101, 6396, 2335, 1024, 1005, 3053, 2035, 2317...\n",
       "4       [101, 19607, 2319, 1999, 9371, 1024, 2647, 417...\n",
       "                              ...                        \n",
       "9995    [101, 1030, 2168, 2078, 14028, 2017, 4895, 148...\n",
       "9996    [101, 1030, 4907, 15202, 3630, 6392, 2015, 235...\n",
       "9997    [101, 1030, 1016, 4783, 26295, 2102, 19205, 30...\n",
       "9998    [101, 1045, 5223, 7743, 2229, 2040, 2831, 2055...\n",
       "9999    [101, 1030, 5754, 3597, 11314, 2121, 1030, 622...\n",
       "Name: text_bert, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_dev.text_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 45 at dim 1 (got 76)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-650f346f1437>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Convert inputs to PyTorch tensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtokens_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_train_dev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_bert\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_train_dev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_bert\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0msegments_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msegments_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegments_ids_1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 45 at dim 1 (got 76)"
     ]
    }
   ],
   "source": [
    "# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
    "segments_ids = [1] * len(df_train_dev.text_bert[0])\n",
    "segments_ids_1 = [1] * len(df_train_dev.text_bert[1])\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([df_train_dev.text_bert[0]])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n",
      "PROCESS\n"
     ]
    }
   ],
   "source": [
    "train_vectors = []\n",
    "test_vectors = []\n",
    "counter = 0\n",
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "    for text in df_train_dev.text_bert:\n",
    "        if counter % 100 == 0:\n",
    "            print(\"PROCESS\")\n",
    "        counter += 1\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([text])\n",
    "        segments_tensors = torch.tensor([[1] * len(text)])\n",
    "        \n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "        train_vectors.append(torch.mean(hidden_states[-2][0], dim=0).tolist())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Evaluating the model will return a different number of objects based on \n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "    # becase we set `output_hidden_states = True`, the third item will be the \n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    #hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESS: 0\n",
      "PROCESS: 200\n",
      "PROCESS: 400\n",
      "PROCESS: 600\n",
      "PROCESS: 800\n",
      "PROCESS: 1000\n",
      "PROCESS: 1200\n",
      "PROCESS: 1400\n",
      "PROCESS: 1600\n",
      "PROCESS: 1800\n",
      "PROCESS: 2000\n",
      "PROCESS: 2200\n",
      "PROCESS: 2400\n",
      "PROCESS: 2600\n",
      "PROCESS: 2800\n"
     ]
    }
   ],
   "source": [
    "test_vectors = []\n",
    "counter = 0\n",
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "    for text in df_test.text_bert:\n",
    "        if counter % 200 == 0:\n",
    "            print(\"PROCESS:\", counter)\n",
    "        counter += 1\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([text])\n",
    "        segments_tensors = torch.tensor([[1] * len(text)])\n",
    "        \n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "        test_vectors.append(torch.mean(hidden_states[-2][0], dim=0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESS: 0\n",
      "PROCESS: 200\n",
      "PROCESS: 400\n",
      "PROCESS: 600\n",
      "PROCESS: 800\n",
      "PROCESS: 1000\n",
      "PROCESS: 1200\n",
      "PROCESS: 1400\n",
      "PROCESS: 1600\n",
      "PROCESS: 1800\n",
      "PROCESS: 2000\n",
      "PROCESS: 2200\n",
      "PROCESS: 2400\n",
      "PROCESS: 2600\n",
      "PROCESS: 2800\n",
      "PROCESS: 3000\n",
      "PROCESS: 3200\n",
      "PROCESS: 3400\n",
      "PROCESS: 3600\n",
      "PROCESS: 3800\n",
      "PROCESS: 4000\n",
      "PROCESS: 4200\n",
      "PROCESS: 4400\n",
      "PROCESS: 4600\n",
      "PROCESS: 4800\n",
      "PROCESS: 5000\n",
      "PROCESS: 5200\n",
      "PROCESS: 5400\n",
      "PROCESS: 5600\n",
      "PROCESS: 5800\n",
      "PROCESS: 6000\n",
      "PROCESS: 6200\n",
      "PROCESS: 6400\n",
      "PROCESS: 6600\n",
      "PROCESS: 6800\n",
      "PROCESS: 7000\n",
      "PROCESS: 7200\n",
      "PROCESS: 7400\n",
      "PROCESS: 7600\n",
      "PROCESS: 7800\n",
      "PROCESS: 8000\n",
      "PROCESS: 8200\n",
      "PROCESS: 8400\n",
      "PROCESS: 8600\n",
      "PROCESS: 8800\n",
      "PROCESS: 9000\n",
      "PROCESS: 9200\n",
      "PROCESS: 9400\n",
      "PROCESS: 9600\n",
      "PROCESS: 9800\n"
     ]
    }
   ],
   "source": [
    "train_vectors_cleaned = []\n",
    "counter = 0\n",
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "    for text in df_train_dev.text_cleaned_bert:\n",
    "        if counter % 200 == 0:\n",
    "            print(\"PROCESS:\", counter)\n",
    "        counter += 1\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([text])\n",
    "        segments_tensors = torch.tensor([[1] * len(text)])\n",
    "        \n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "        train_vectors_cleaned.append(torch.mean(hidden_states[-2][0], dim=0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESS: 0\n",
      "PROCESS: 200\n",
      "PROCESS: 400\n",
      "PROCESS: 600\n",
      "PROCESS: 800\n",
      "PROCESS: 1000\n",
      "PROCESS: 1200\n",
      "PROCESS: 1400\n",
      "PROCESS: 1600\n",
      "PROCESS: 1800\n",
      "PROCESS: 2000\n",
      "PROCESS: 2200\n",
      "PROCESS: 2400\n",
      "PROCESS: 2600\n",
      "PROCESS: 2800\n"
     ]
    }
   ],
   "source": [
    "test_vectors_cleaned = []\n",
    "counter = 0\n",
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "    for text in df_test.text_cleaned_bert:\n",
    "        if counter % 200 == 0:\n",
    "            print(\"PROCESS:\", counter)\n",
    "        counter += 1\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([text])\n",
    "        segments_tensors = torch.tensor([[1] * len(text)])\n",
    "        \n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "        test_vectors_cleaned.append(torch.mean(hidden_states[-2][0], dim=0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train_dev.HS\n",
    "y_test = df_test.HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from evaluate.ipynb\n",
      "taskA_fscore: 0.5462134785788648\n",
      "taskA_precision: 0.6515250130962755\n",
      "taskA_recall: 0.6093732895457034\n",
      "taskA_accuracy: 0.563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import evaluate # here we import the local evaluate.ipynb jupyter notebook\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Evaluate using Logistic Regression as the classifier (without normalized data)\n",
    "\n",
    "logreg = LogisticRegression().fit(train_vectors, y_train)\n",
    "y_predict = logreg.predict(test_vectors)\n",
    "\n",
    "# Create new test dataframe\n",
    "df_test_bert = df_test.copy()\n",
    "df_test_bert['HS'] = y_predict\n",
    "\n",
    "# Create prediction file for the bert\n",
    "df_test_bert[['id', 'HS']].to_csv('predictions/bert.tsv', sep='\\t', index=False, header=False)\n",
    "df_test_bert[['id', 'HS']].to_csv('input/res/en_a.tsv', sep='\\t', index=False, header=False)\n",
    "\n",
    "# Evaluate the result of the bert\n",
    "evaluate.write_eval(\"scores_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taskA_fscore: 0.5007588161507743\n",
      "taskA_precision: 0.6206242320015312\n",
      "taskA_recall: 0.5773262178434593\n",
      "taskA_accuracy: 0.5263333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import evaluate # here we import the local evaluate.ipynb jupyter notebook\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Evaluate using Logistic Regression as the classifier (without normalized data)\n",
    "\n",
    "logreg = LogisticRegression().fit(train_vectors_cleaned, y_train)\n",
    "y_predict_cleaned = logreg.predict(test_vectors_cleaned)\n",
    "\n",
    "# Create new test dataframe\n",
    "df_test_bert_cleaned = df_test.copy()\n",
    "df_test_bert_cleaned['HS'] = y_predict_cleaned\n",
    "\n",
    "# Create prediction file for the bert_cleaned\n",
    "df_test_bert_cleaned[['id', 'HS']].to_csv('predictions/bert_cleaned.tsv', sep='\\t', index=False, header=False)\n",
    "df_test_bert_cleaned[['id', 'HS']].to_csv('input/res/en_a.tsv', sep='\\t', index=False, header=False)\n",
    "\n",
    "# Evaluate the result of the bert_cleaned\n",
    "evaluate.write_eval(\"scores_bert_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Normalize the data via StandardScaler\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(train_vectors)\n",
    "train_vectors_scaled = scaler.transform(train_vectors)\n",
    "test_vectors_scaled = scaler.transform(test_vectors)\n",
    "\n",
    "scaler_cleaned = preprocessing.StandardScaler().fit(train_vectors_cleaned)\n",
    "train_vectors_scaled_cleaned = scaler_cleaned.transform(train_vectors_cleaned)\n",
    "test_vectors_scaled_cleaned = scaler_cleaned.transform(test_vectors_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taskA_fscore: 0.5475686130042365\n",
      "taskA_precision: 0.6481366459627329\n",
      "taskA_recall: 0.608784893267652\n",
      "taskA_accuracy: 0.5633333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using Logistic Regression as the classifier (without normalized data)\n",
    "\n",
    "logreg = LogisticRegression().fit(train_vectors_scaled, y_train)\n",
    "y_predict = logreg.predict(test_vectors_scaled)\n",
    "\n",
    "# Create new test dataframe\n",
    "df_test_bert_scaled = df_test.copy()\n",
    "df_test_bert_scaled['HS'] = y_predict\n",
    "\n",
    "# Create prediction file for the bert_scaled\n",
    "df_test_bert_scaled[['id', 'HS']].to_csv('predictions/bert_scaled.tsv', sep='\\t', index=False, header=False)\n",
    "df_test_bert_scaled[['id', 'HS']].to_csv('input/res/en_a.tsv', sep='\\t', index=False, header=False)\n",
    "\n",
    "# Evaluate the result of the bert_scaled\n",
    "evaluate.write_eval(\"scores_bert_scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taskA_fscore: 0.5004151512670885\n",
      "taskA_precision: 0.6168858569519657\n",
      "taskA_recall: 0.5758073344280241\n",
      "taskA_accuracy: 0.5253333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using Logistic Regression as the classifier (without normalized data)\n",
    "\n",
    "logreg = LogisticRegression().fit(train_vectors_scaled_cleaned, y_train)\n",
    "y_predict = logreg.predict(test_vectors_scaled_cleaned)\n",
    "\n",
    "# Create new test dataframe\n",
    "df_test_bert_scaled_cleaned = df_test.copy()\n",
    "df_test_bert_scaled_cleaned['HS'] = y_predict\n",
    "\n",
    "# Create prediction file for the bert_scaled_cleaned\n",
    "df_test_bert_scaled_cleaned[['id', 'HS']].to_csv('predictions/bert_scaled_cleaned.tsv', sep='\\t', index=False, header=False)\n",
    "df_test_bert_scaled_cleaned[['id', 'HS']].to_csv('input/res/en_a.tsv', sep='\\t', index=False, header=False)\n",
    "\n",
    "# Evaluate the result of the bert_scaled_cleaned\n",
    "evaluate.write_eval(\"scores_bert_scaled_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 45, 768])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6069624423980713,\n",
       " 0.2419789433479309,\n",
       " 0.7163507342338562,\n",
       " 0.06172146275639534,\n",
       " 0.4696476459503174,\n",
       " -0.149113729596138,\n",
       " 0.05396163836121559,\n",
       " 0.2692813575267792,\n",
       " -0.5679747462272644,\n",
       " -0.37784314155578613,\n",
       " 0.07539860159158707,\n",
       " -0.3541121780872345,\n",
       " 0.03370343893766403,\n",
       " 0.5155255794525146,\n",
       " 0.054346948862075806,\n",
       " 0.9002467393875122,\n",
       " -0.23749478161334991,\n",
       " 0.4720906615257263,\n",
       " -0.33111461997032166,\n",
       " 0.3972581624984741,\n",
       " 0.7518365383148193,\n",
       " 0.26511290669441223,\n",
       " -0.14997993409633636,\n",
       " 0.016306163743138313,\n",
       " 0.38244205713272095,\n",
       " -0.09090139716863632,\n",
       " -0.0763014554977417,\n",
       " -0.6351617574691772,\n",
       " -0.7715164422988892,\n",
       " 0.12208463996648788,\n",
       " 0.2259512096643448,\n",
       " 0.2197011262178421,\n",
       " 0.6285077333450317,\n",
       " -0.2671724855899811,\n",
       " -0.29905569553375244,\n",
       " -0.6454766392707825,\n",
       " -0.3351753354072571,\n",
       " -0.2363114058971405,\n",
       " 0.6731697916984558,\n",
       " 0.3784375786781311,\n",
       " -0.1486649364233017,\n",
       " -0.23532456159591675,\n",
       " 0.22471709549427032,\n",
       " -0.005852035712450743,\n",
       " -0.10157843679189682,\n",
       " 0.23649147152900696,\n",
       " 0.043905243277549744,\n",
       " -0.46387794613838196,\n",
       " -0.038600899279117584,\n",
       " -0.5210039615631104,\n",
       " -0.7611696124076843,\n",
       " 0.05742819979786873,\n",
       " 0.23330537974834442,\n",
       " -0.6544169187545776,\n",
       " 0.23178759217262268,\n",
       " 0.4381251037120819,\n",
       " 0.31985852122306824,\n",
       " -0.544605553150177,\n",
       " -0.15934380888938904,\n",
       " 0.055209338665008545,\n",
       " -0.45097625255584717,\n",
       " -0.4471879303455353,\n",
       " 0.15000249445438385,\n",
       " 0.27756914496421814,\n",
       " -0.2345827966928482,\n",
       " -0.09526295214891434,\n",
       " 0.0027658885810524225,\n",
       " 0.7801222205162048,\n",
       " -0.9119019508361816,\n",
       " -0.1079825684428215,\n",
       " 0.1743602603673935,\n",
       " -0.12579558789730072,\n",
       " -0.03730334714055061,\n",
       " 0.2777467370033264,\n",
       " 0.03257608041167259,\n",
       " 0.0981559082865715,\n",
       " -0.1345466822385788,\n",
       " 0.7150644659996033,\n",
       " 0.12361127883195877,\n",
       " -0.03247003257274628,\n",
       " -0.12290123850107193,\n",
       " 0.08976669609546661,\n",
       " 0.39665576815605164,\n",
       " 0.1853571981191635,\n",
       " 0.5349606871604919,\n",
       " 0.11588364839553833,\n",
       " 0.351650208234787,\n",
       " 0.16175082325935364,\n",
       " -0.6465609073638916,\n",
       " 0.49894556403160095,\n",
       " -0.11495622247457504,\n",
       " 0.0767950564622879,\n",
       " -0.08753406256437302,\n",
       " 0.04477772116661072,\n",
       " 0.8572496771812439,\n",
       " -0.4617260694503784,\n",
       " -0.7531529068946838,\n",
       " -0.4268971383571625,\n",
       " -0.09081622958183289,\n",
       " 0.18353596329689026,\n",
       " 0.45667320489883423,\n",
       " -0.6937588453292847,\n",
       " 0.04902218282222748,\n",
       " -0.3806265592575073,\n",
       " -0.3115100562572479,\n",
       " -0.2242630571126938,\n",
       " -0.4035761058330536,\n",
       " 0.07580378651618958,\n",
       " 0.24752742052078247,\n",
       " -0.001060957321897149,\n",
       " 0.19365194439888,\n",
       " 0.28811854124069214,\n",
       " 0.15995192527770996,\n",
       " -0.6871770024299622,\n",
       " 0.14359895884990692,\n",
       " -0.2914119362831116,\n",
       " -0.22991874814033508,\n",
       " 0.6096128821372986,\n",
       " 0.5125834345817566,\n",
       " -0.26492050290107727,\n",
       " -0.3165643811225891,\n",
       " 0.5037441849708557,\n",
       " 0.4082074761390686,\n",
       " 0.2228868454694748,\n",
       " 0.19495277106761932,\n",
       " 0.3285667300224304,\n",
       " -0.30413705110549927,\n",
       " -0.12369035184383392,\n",
       " 0.13292397558689117,\n",
       " 0.6409038305282593,\n",
       " 0.6428349614143372,\n",
       " 0.617125928401947,\n",
       " -0.1252281665802002,\n",
       " -0.3233623206615448,\n",
       " -0.10157220810651779,\n",
       " 0.07012370228767395,\n",
       " 0.2525656521320343,\n",
       " -0.01989114098250866,\n",
       " -0.08187507838010788,\n",
       " -0.4326377809047699,\n",
       " -0.3766552805900574,\n",
       " -0.22845259308815002,\n",
       " 0.3176077604293823,\n",
       " 0.41478589177131653,\n",
       " -0.11552261561155319,\n",
       " -0.0678647980093956,\n",
       " -0.11175545305013657,\n",
       " -0.07350177317857742,\n",
       " -0.16038645803928375,\n",
       " 0.8903144001960754,\n",
       " -0.2728199064731598,\n",
       " 0.05264892429113388,\n",
       " -0.5233458876609802,\n",
       " -0.09281085431575775,\n",
       " 0.16415700316429138,\n",
       " -0.1945618987083435,\n",
       " 0.04499957710504532,\n",
       " -0.1124458983540535,\n",
       " 0.41808751225471497,\n",
       " 0.5205241441726685,\n",
       " 0.21136024594306946,\n",
       " -0.30270877480506897,\n",
       " 0.0061775920912623405,\n",
       " 0.155549094080925,\n",
       " 0.1636214256286621,\n",
       " -0.4064466655254364,\n",
       " 0.016572248190641403,\n",
       " 0.09357050806283951,\n",
       " 0.3963977098464966,\n",
       " -0.049790091812610626,\n",
       " 0.23256321251392365,\n",
       " -0.9104853868484497,\n",
       " 0.09317310899496078,\n",
       " 0.4597639739513397,\n",
       " 0.42866286635398865,\n",
       " 0.46737298369407654,\n",
       " 0.357529878616333,\n",
       " -0.5412179827690125,\n",
       " 0.22175242006778717,\n",
       " 1.1211355924606323,\n",
       " -0.3410307765007019,\n",
       " -0.1325426697731018,\n",
       " 0.4088440239429474,\n",
       " -0.14194968342781067,\n",
       " 0.06122490391135216,\n",
       " -0.03034825064241886,\n",
       " 0.356570303440094,\n",
       " -0.22444303333759308,\n",
       " 0.726511538028717,\n",
       " 0.4660544991493225,\n",
       " -0.6946802735328674,\n",
       " -0.20516838133335114,\n",
       " 0.2547183930873871,\n",
       " -0.3383786380290985,\n",
       " 0.7369344234466553,\n",
       " -0.1760311722755432,\n",
       " -0.04051116853952408,\n",
       " -0.29167789220809937,\n",
       " 0.7115687131881714,\n",
       " 0.210285484790802,\n",
       " 0.3405194580554962,\n",
       " 0.3010370135307312,\n",
       " -0.07598520815372467,\n",
       " -0.27978938817977905,\n",
       " -0.5197989344596863,\n",
       " 0.5236721634864807,\n",
       " -0.10691572725772858,\n",
       " -0.21080319583415985,\n",
       " -1.1062761545181274,\n",
       " 0.11995815485715866,\n",
       " -0.2844260334968567,\n",
       " 0.3594386577606201,\n",
       " 0.04303283244371414,\n",
       " 0.25570064783096313,\n",
       " -0.23527447879314423,\n",
       " -0.05324052646756172,\n",
       " 0.013461565598845482,\n",
       " -0.5426749587059021,\n",
       " 0.32968616485595703,\n",
       " 0.2126777172088623,\n",
       " -0.5945716500282288,\n",
       " -0.3574380576610565,\n",
       " 0.03724345192313194,\n",
       " -0.0786464512348175,\n",
       " -0.48611921072006226,\n",
       " 1.3731929063796997,\n",
       " -0.012935306876897812,\n",
       " -0.7509890198707581,\n",
       " 0.6574100852012634,\n",
       " -0.17926903069019318,\n",
       " 0.07048263400793076,\n",
       " 0.12706488370895386,\n",
       " -0.1389041543006897,\n",
       " -0.2895161509513855,\n",
       " 0.7831913232803345,\n",
       " 0.224742591381073,\n",
       " 0.07058944553136826,\n",
       " -0.2485760599374771,\n",
       " -0.05478763207793236,\n",
       " 0.3141930103302002,\n",
       " -0.45035049319267273,\n",
       " 0.8513455986976624,\n",
       " 0.09586449712514877,\n",
       " -0.013866975903511047,\n",
       " -0.4594126343727112,\n",
       " 0.3348846435546875,\n",
       " 0.045845773071050644,\n",
       " -0.2539506256580353,\n",
       " -0.09636019915342331,\n",
       " 0.013014833442866802,\n",
       " -0.06686025112867355,\n",
       " 0.32438305020332336,\n",
       " -0.13715220987796783,\n",
       " -0.5035324692726135,\n",
       " -0.036002494394779205,\n",
       " -0.5587244033813477,\n",
       " 0.10465887933969498,\n",
       " -0.37428030371665955,\n",
       " -0.6438671350479126,\n",
       " 0.40617120265960693,\n",
       " 0.4534444212913513,\n",
       " 0.22686882317066193,\n",
       " -0.1338944286108017,\n",
       " -0.051638901233673096,\n",
       " 0.4835021197795868,\n",
       " -0.28721749782562256,\n",
       " -0.08668383210897446,\n",
       " 0.2746719419956207,\n",
       " 0.7816260457038879,\n",
       " 0.36508709192276,\n",
       " -0.5480238795280457,\n",
       " 0.1539163738489151,\n",
       " 0.031494539231061935,\n",
       " 0.5015371441841125,\n",
       " -0.19643308222293854,\n",
       " -0.5023070573806763,\n",
       " -0.02096378803253174,\n",
       " 0.15607278048992157,\n",
       " -0.02459706924855709,\n",
       " -0.2166948914527893,\n",
       " -0.40894147753715515,\n",
       " 0.6866353154182434,\n",
       " -0.2910788357257843,\n",
       " 0.19539467990398407,\n",
       " -0.008725532330572605,\n",
       " -0.2606073021888733,\n",
       " 0.4352782368659973,\n",
       " 0.10362178087234497,\n",
       " -0.4250224530696869,\n",
       " 0.40192270278930664,\n",
       " -0.19401849806308746,\n",
       " -0.14912903308868408,\n",
       " -0.17878924310207367,\n",
       " -0.02450074627995491,\n",
       " 0.905853271484375,\n",
       " 0.25169023871421814,\n",
       " -0.09032129496335983,\n",
       " 0.07432014495134354,\n",
       " -0.3539540767669678,\n",
       " 0.047133758664131165,\n",
       " -0.11209123581647873,\n",
       " -0.07480572164058685,\n",
       " 0.05243458226323128,\n",
       " 0.0710749551653862,\n",
       " 0.20809805393218994,\n",
       " -0.29061105847358704,\n",
       " -0.6331236958503723,\n",
       " -0.14998866617679596,\n",
       " -8.909472465515137,\n",
       " -0.35051313042640686,\n",
       " -0.08253175765275955,\n",
       " -0.2800547778606415,\n",
       " -0.044047486037015915,\n",
       " -0.3151695728302002,\n",
       " -0.24068044126033783,\n",
       " 0.09902755916118622,\n",
       " -0.22832956910133362,\n",
       " -0.3318568170070648,\n",
       " -0.06044825538992882,\n",
       " -0.11406389623880386,\n",
       " -0.16509641706943512,\n",
       " -0.43204420804977417,\n",
       " -0.35906800627708435,\n",
       " -0.11791913956403732,\n",
       " 0.4475209414958954,\n",
       " 0.42944949865341187,\n",
       " -0.3252946734428406,\n",
       " 0.19938774406909943,\n",
       " -0.9228955507278442,\n",
       " 0.04984899237751961,\n",
       " 0.5514128804206848,\n",
       " 0.2895652651786804,\n",
       " 0.955247163772583,\n",
       " -0.4683745503425598,\n",
       " -0.23699204623699188,\n",
       " -0.015895791351795197,\n",
       " -0.1569819301366806,\n",
       " -0.23539669811725616,\n",
       " 0.02508089877665043,\n",
       " 0.17046797275543213,\n",
       " 0.09668480604887009,\n",
       " 1.243795394897461,\n",
       " -0.3487693667411804,\n",
       " -0.10883602499961853,\n",
       " 0.08903990685939789,\n",
       " -0.8494709730148315,\n",
       " -0.4595124125480652,\n",
       " -0.4683343768119812,\n",
       " 0.05767694115638733,\n",
       " -0.7948620319366455,\n",
       " -0.377896785736084,\n",
       " -0.19398227334022522,\n",
       " 0.8067479729652405,\n",
       " -0.5282272100448608,\n",
       " -0.2805374562740326,\n",
       " 0.01190947461873293,\n",
       " 0.1618601232767105,\n",
       " 0.12881046533584595,\n",
       " -0.28678274154663086,\n",
       " -0.18151570856571198,\n",
       " -0.004262214060872793,\n",
       " -0.539375364780426,\n",
       " -0.10602805763483047,\n",
       " -0.1242608055472374,\n",
       " 0.14946453273296356,\n",
       " 0.7861967086791992,\n",
       " -0.30371397733688354,\n",
       " -0.19866150617599487,\n",
       " 0.49871504306793213,\n",
       " -0.1837698370218277,\n",
       " -0.4718179404735565,\n",
       " 0.2422042042016983,\n",
       " -0.14187310636043549,\n",
       " -0.3395606279373169,\n",
       " -0.6418672204017639,\n",
       " -0.46261394023895264,\n",
       " -0.5707963109016418,\n",
       " -0.15491989254951477,\n",
       " -0.410984069108963,\n",
       " 0.38206911087036133,\n",
       " -0.16470099985599518,\n",
       " -0.9915100932121277,\n",
       " -0.13259544968605042,\n",
       " -0.22135788202285767,\n",
       " -0.006852470338344574,\n",
       " 0.12747876346111298,\n",
       " -0.22253015637397766,\n",
       " -0.31103643774986267,\n",
       " -1.0232831239700317,\n",
       " 0.0480206273496151,\n",
       " 0.10090554505586624,\n",
       " -0.22023943066596985,\n",
       " -0.4549633860588074,\n",
       " -0.32127439975738525,\n",
       " -0.21878233551979065,\n",
       " -0.690829873085022,\n",
       " -0.5502204895019531,\n",
       " 0.7047901749610901,\n",
       " -0.1730649769306183,\n",
       " 0.18573807179927826,\n",
       " 0.12203919887542725,\n",
       " -0.0008247746154665947,\n",
       " 0.4121033549308777,\n",
       " -0.3109472990036011,\n",
       " 0.3195069134235382,\n",
       " -0.1834190934896469,\n",
       " -0.06932909041643143,\n",
       " 0.20918869972229004,\n",
       " 0.17072556912899017,\n",
       " -0.08685947209596634,\n",
       " 0.6931792497634888,\n",
       " -0.13939014077186584,\n",
       " -0.32125237584114075,\n",
       " 0.1507946252822876,\n",
       " -0.4523109495639801,\n",
       " 0.49506819248199463,\n",
       " -0.009345863945782185,\n",
       " -0.11842897534370422,\n",
       " -0.18501412868499756,\n",
       " 0.4523462951183319,\n",
       " -0.37776979804039,\n",
       " -0.2939586341381073,\n",
       " -0.7187029719352722,\n",
       " -0.1498032659292221,\n",
       " 0.19667063653469086,\n",
       " 0.5149457454681396,\n",
       " 0.21104130148887634,\n",
       " -0.8963378667831421,\n",
       " -0.5827998518943787,\n",
       " 0.3120580017566681,\n",
       " -0.3834228813648224,\n",
       " -0.33589521050453186,\n",
       " 0.18867777287960052,\n",
       " 0.0642477422952652,\n",
       " 0.10345122218132019,\n",
       " 0.055332645773887634,\n",
       " 0.011282532475888729,\n",
       " -0.021500682458281517,\n",
       " -0.28427818417549133,\n",
       " -0.6379246115684509,\n",
       " -0.3456208407878876,\n",
       " -0.6656361222267151,\n",
       " 0.05524509400129318,\n",
       " -0.3594214618206024,\n",
       " 0.23151849210262299,\n",
       " -0.40072953701019287,\n",
       " -0.45523130893707275,\n",
       " 0.2397100180387497,\n",
       " 0.5968142747879028,\n",
       " 0.16628876328468323,\n",
       " 0.17396371066570282,\n",
       " 0.04023687541484833,\n",
       " 0.05468283221125603,\n",
       " -0.006558308377861977,\n",
       " 0.061080604791641235,\n",
       " 0.11902640759944916,\n",
       " -0.07936827838420868,\n",
       " 0.42936086654663086,\n",
       " -0.5609363913536072,\n",
       " -0.916336715221405,\n",
       " 0.13900740444660187,\n",
       " -0.39455482363700867,\n",
       " 0.28617429733276367,\n",
       " -0.09996015578508377,\n",
       " 0.6282641887664795,\n",
       " -0.5119335651397705,\n",
       " -0.43819791078567505,\n",
       " -0.44642019271850586,\n",
       " 0.27085572481155396,\n",
       " -0.18243493139743805,\n",
       " -0.3986213803291321,\n",
       " 0.052838586270809174,\n",
       " 0.47075632214546204,\n",
       " -0.35470524430274963,\n",
       " 0.2697070240974426,\n",
       " 0.11152583360671997,\n",
       " -0.24595798552036285,\n",
       " -1.0642932653427124,\n",
       " -0.33289897441864014,\n",
       " 0.6170234680175781,\n",
       " 0.18846094608306885,\n",
       " 0.13675682246685028,\n",
       " 0.04064927250146866,\n",
       " 0.22531725466251373,\n",
       " 0.23572270572185516,\n",
       " -0.34922194480895996,\n",
       " 0.04733552038669586,\n",
       " 0.5639578700065613,\n",
       " 0.2640538811683655,\n",
       " 0.5003246665000916,\n",
       " 0.1921089142560959,\n",
       " -0.030716771259903908,\n",
       " 0.23888427019119263,\n",
       " -0.180380716919899,\n",
       " 0.24033917486667633,\n",
       " 0.23680369555950165,\n",
       " 0.6060816645622253,\n",
       " 0.4821108877658844,\n",
       " -0.8371354937553406,\n",
       " 0.7816869020462036,\n",
       " 0.05177785083651543,\n",
       " -0.4167115092277527,\n",
       " 0.26845112442970276,\n",
       " -0.690416693687439,\n",
       " 0.49408188462257385,\n",
       " -0.35690009593963623,\n",
       " -0.09116343408823013,\n",
       " 0.4580628275871277,\n",
       " -0.29029497504234314,\n",
       " -0.1425468623638153,\n",
       " -0.7617999315261841,\n",
       " 0.09079626947641373,\n",
       " -0.497055321931839,\n",
       " 0.2680457532405853,\n",
       " 0.24207155406475067,\n",
       " 0.3096531927585602,\n",
       " 0.595509946346283,\n",
       " 0.20095288753509521,\n",
       " -0.024645891040563583,\n",
       " -0.25583818554878235,\n",
       " -0.9950976967811584,\n",
       " -0.1496928483247757,\n",
       " 0.1946365088224411,\n",
       " -0.5467833876609802,\n",
       " -0.0797080248594284,\n",
       " -0.4329893887042999,\n",
       " -0.6092010140419006,\n",
       " 0.11603283137083054,\n",
       " -0.05800085514783859,\n",
       " -0.14187346398830414,\n",
       " 0.04493921995162964,\n",
       " -0.027696233242750168,\n",
       " -0.04068274423480034,\n",
       " 0.5225904583930969,\n",
       " 0.019102636724710464,\n",
       " -0.3009360134601593,\n",
       " -0.03418338671326637,\n",
       " -0.22916480898857117,\n",
       " 0.2595095634460449,\n",
       " -1.9520187377929688,\n",
       " 0.26539403200149536,\n",
       " 0.1673993468284607,\n",
       " 0.05894426628947258,\n",
       " -0.05046583339571953,\n",
       " -0.2630186080932617,\n",
       " 0.2703060507774353,\n",
       " -0.34239429235458374,\n",
       " -0.09449323266744614,\n",
       " -0.1819620132446289,\n",
       " -0.6834605932235718,\n",
       " -0.1279267519712448,\n",
       " -0.8177322149276733,\n",
       " -0.19021622836589813,\n",
       " 0.09017367660999298,\n",
       " -0.6260442733764648,\n",
       " 0.2220507115125656,\n",
       " -0.36912667751312256,\n",
       " -0.23518483340740204,\n",
       " 0.335522323846817,\n",
       " -0.43514513969421387,\n",
       " -0.3097703158855438,\n",
       " -0.6505117416381836,\n",
       " 0.317476361989975,\n",
       " -0.7141671776771545,\n",
       " -0.3405679762363434,\n",
       " -0.25537243485450745,\n",
       " 0.00602373480796814,\n",
       " -0.04829677939414978,\n",
       " -0.296525239944458,\n",
       " -0.05932599678635597,\n",
       " -0.21512147784233093,\n",
       " 0.0470256470143795,\n",
       " -0.05169108882546425,\n",
       " 0.10223519802093506,\n",
       " 0.32385411858558655,\n",
       " -0.019326549023389816,\n",
       " 0.05179660767316818,\n",
       " -0.892667829990387,\n",
       " -0.3695995807647705,\n",
       " -0.21174472570419312,\n",
       " -0.23029810190200806,\n",
       " 0.4579760730266571,\n",
       " -0.21174706518650055,\n",
       " -0.5372684597969055,\n",
       " -0.408760666847229,\n",
       " -0.2055533081293106,\n",
       " 0.5332219004631042,\n",
       " -0.5187557935714722,\n",
       " -0.366252064704895,\n",
       " -0.4370112419128418,\n",
       " 0.16624291241168976,\n",
       " 0.06934183835983276,\n",
       " 0.0975394994020462,\n",
       " -0.25610408186912537,\n",
       " -0.24739928543567657,\n",
       " -0.0014294200809672475,\n",
       " 0.024509800598025322,\n",
       " -0.124884232878685,\n",
       " 0.1796780228614807,\n",
       " 0.09266911447048187,\n",
       " -0.10240955650806427,\n",
       " -0.0005994094535708427,\n",
       " -0.047792647033929825,\n",
       " 0.46053433418273926,\n",
       " 0.6725120544433594,\n",
       " 0.017103325575590134,\n",
       " 0.25448283553123474,\n",
       " -0.03191658854484558,\n",
       " -0.2940394878387451,\n",
       " -0.4060322642326355,\n",
       " 0.6590914726257324,\n",
       " 0.14911821484565735,\n",
       " 0.12604984641075134,\n",
       " -0.101619191467762,\n",
       " 0.25991910696029663,\n",
       " -0.25253012776374817,\n",
       " -0.2719767689704895,\n",
       " -0.4784894287586212,\n",
       " -0.7773285508155823,\n",
       " -0.25512856245040894,\n",
       " 0.7630810141563416,\n",
       " 0.5081387162208557,\n",
       " -0.14330242574214935,\n",
       " -0.2640979588031769,\n",
       " -0.528619647026062,\n",
       " -0.171111598610878,\n",
       " -0.31410473585128784,\n",
       " -0.12786103785037994,\n",
       " -0.5072767734527588,\n",
       " 0.14494961500167847,\n",
       " 0.6672428846359253,\n",
       " -0.40415966510772705,\n",
       " 0.1268053650856018,\n",
       " 0.42352795600891113,\n",
       " 0.25941044092178345,\n",
       " -0.3823751211166382,\n",
       " 0.36317020654678345,\n",
       " -0.09148018807172775,\n",
       " -0.5869672298431396,\n",
       " 0.219806969165802,\n",
       " 0.2744789123535156,\n",
       " 0.1309840977191925,\n",
       " 0.13459232449531555,\n",
       " -0.3672351539134979,\n",
       " 0.11272191256284714,\n",
       " -0.1543722152709961,\n",
       " -0.19394730031490326,\n",
       " 0.02577759511768818,\n",
       " 0.19967897236347198,\n",
       " 0.1493910402059555,\n",
       " -0.4528609812259674,\n",
       " 0.19699759781360626,\n",
       " 0.16793857514858246,\n",
       " 0.08580093085765839,\n",
       " -0.08798792213201523,\n",
       " 0.19435517489910126,\n",
       " 0.3530951738357544,\n",
       " 0.4898373782634735,\n",
       " -0.18424460291862488,\n",
       " -0.14775213599205017,\n",
       " 0.3680729269981384,\n",
       " 0.3613147735595703,\n",
       " 0.28138163685798645,\n",
       " -0.0697525218129158,\n",
       " 0.17923885583877563,\n",
       " 0.1802779585123062,\n",
       " 0.30334728956222534,\n",
       " 0.69306880235672,\n",
       " 0.5227046012878418,\n",
       " 0.6317947506904602,\n",
       " -0.09819088131189346,\n",
       " -0.3225683867931366,\n",
       " -0.14256492257118225,\n",
       " 0.12066396325826645,\n",
       " 0.2231629192829132,\n",
       " 0.38583672046661377,\n",
       " 0.061621442437171936,\n",
       " 0.2922576665878296,\n",
       " 0.4251691997051239,\n",
       " -0.05084458366036415,\n",
       " 0.5571261644363403,\n",
       " 0.2683100998401642,\n",
       " -0.19731400907039642,\n",
       " 0.3098536729812622,\n",
       " 0.17449384927749634,\n",
       " -0.23900333046913147,\n",
       " -0.07632574439048767,\n",
       " -0.2591875195503235,\n",
       " -0.14559748768806458,\n",
       " 0.11970438063144684,\n",
       " -0.11541848629713058,\n",
       " -0.06893546879291534,\n",
       " 0.17331229150295258,\n",
       " -0.2751612663269043,\n",
       " -0.07748692482709885,\n",
       " -0.09288480877876282,\n",
       " -0.2003037929534912,\n",
       " -0.3521644175052643,\n",
       " -0.14554132521152496,\n",
       " 0.22660931944847107,\n",
       " 0.2572597563266754,\n",
       " 0.2340555638074875,\n",
       " 0.05456501990556717,\n",
       " -0.050647638738155365,\n",
       " -0.07684893906116486,\n",
       " 0.09830367565155029,\n",
       " 0.3782801032066345,\n",
       " -0.04806216433644295,\n",
       " 0.039201412349939346,\n",
       " -0.1448362171649933,\n",
       " -0.21809308230876923,\n",
       " -0.09610184282064438,\n",
       " 0.12248603254556656,\n",
       " -0.13218320906162262,\n",
       " -0.3584709167480469,\n",
       " 0.049665823578834534,\n",
       " -0.6126542091369629,\n",
       " -0.006297230254858732,\n",
       " -0.009227689355611801,\n",
       " 0.2706020474433899,\n",
       " 0.0019684035796672106,\n",
       " 0.4454922080039978,\n",
       " -0.34611740708351135,\n",
       " 0.14572492241859436,\n",
       " 0.2786121964454651,\n",
       " -0.7010884881019592,\n",
       " -0.853470504283905,\n",
       " -0.04828726500272751,\n",
       " -0.139701247215271,\n",
       " -0.40551748871803284,\n",
       " -0.15197822451591492,\n",
       " -0.14929701387882233,\n",
       " 0.05808022990822792,\n",
       " -0.051574379205703735,\n",
       " -0.01874813437461853,\n",
       " 0.07942521572113037,\n",
       " 0.08610847592353821,\n",
       " 0.007347493898123503,\n",
       " 0.17561621963977814,\n",
       " 0.46286270022392273,\n",
       " 0.4259856045246124,\n",
       " -0.05023078992962837,\n",
       " -0.7019139528274536,\n",
       " -0.05097256600856781,\n",
       " -0.015433523803949356,\n",
       " 0.24636724591255188,\n",
       " -0.14662210643291473,\n",
       " 0.6173115372657776,\n",
       " 0.10732989758253098,\n",
       " -0.18025058507919312,\n",
       " -0.01673923246562481,\n",
       " -0.14951680600643158,\n",
       " -0.2900862693786621,\n",
       " 0.305674284696579,\n",
       " 0.46793338656425476,\n",
       " 0.05953788757324219,\n",
       " -0.5311189293861389,\n",
       " -0.5211687088012695,\n",
       " 0.1838105022907257,\n",
       " -0.377341628074646,\n",
       " -0.5256558060646057,\n",
       " -0.1437021791934967,\n",
       " 0.5309275984764099,\n",
       " 0.2282959669828415,\n",
       " -0.2219034880399704,\n",
       " -0.7047713398933411,\n",
       " 0.003397496649995446,\n",
       " 0.25878140330314636]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(hidden_states[-2][0], dim=0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
